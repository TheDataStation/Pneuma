{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword Search (Weighted by BM25)\n",
    "\n",
    "> Pre-requisite: follow [this guide](https://www.elastic.co/guide/en/elasticsearch/reference/current/getting-started.html) to setup self-managed Elasticsearch system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Connect to Elasticsearch Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'name': 'dbd6ede12f79', 'cluster_name': 'docker-cluster', 'cluster_uuid': 'CHnpZicfTCa-YpXISknWzQ', 'version': {'number': '8.13.4', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': 'da95df118650b55a500dcc181889ac35c6d8da7c', 'build_date': '2024-05-06T22:04:45.107454559Z', 'build_snapshot': False, 'lucene_version': '9.10.0', 'minimum_wire_compatibility_version': '7.17.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'You Know, for Search'})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "client = Elasticsearch(\n",
    "  \"https://localhost:9200\",\n",
    "  api_key=\"SEVzTDVvOEJpRjU2U19wVFhrUXM6a204U3lERi1UTUtRNzlQRF9NUENqdw==\",  # Adjust\n",
    "  ca_certs=\"http_ca.crt\"\n",
    ")\n",
    "client.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust, this is an example for summaries.\n",
    "# For context benchmark, no need for summaries.\n",
    "import pandas as pd\n",
    "benchmark = pd.read_csv(\"bx/BX1_public_bi.csv\").fillna(\"\")  # Elasticsearch can't index empty-length strings\n",
    "# summaries = pd.read_csv(\"bc/row_summaries_chicago.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest Contexts/Contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_contexts(benchmark: pd.DataFrame):\n",
    "    client.indices.delete(index='benchmark', ignore=[400, 404])  # Ignore if already removed\n",
    "    for i in range(benchmark.shape[0]):\n",
    "        table = benchmark[\"table\"][i]\n",
    "        context = benchmark[\"context\"][i]\n",
    "        client.index(\n",
    "            index = \"benchmark\",\n",
    "            document={\n",
    "                \"table\": table,\n",
    "                \"context\": context\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_contents(summaries: pd.DataFrame):\n",
    "    client.indices.delete(index='benchmark', ignore=[400, 404])  # Ignore if already removed\n",
    "    for i in range(summaries.shape[0]):\n",
    "        table = summaries[\"table\"][i]\n",
    "        summary = summaries[\"summary\"][i]\n",
    "        client.index(\n",
    "            index = \"benchmark\",\n",
    "            document={\n",
    "                \"table\": table,\n",
    "                \"summary\": summary\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_sample_summaries(summaries: pd.DataFrame, sample_percentage=1):\n",
    "    \"\"\"\n",
    "    This is to get certain percentage of the summaries.\n",
    "    \"\"\"\n",
    "    # Prepare to sample summaries (category refers to the tables)\n",
    "    category_counts = summaries['table'].value_counts()\n",
    "    sample_sizes = np.ceil(category_counts * sample_percentage).astype(int)\n",
    "\n",
    "    # Perform stratified sampling\n",
    "    sampled_summaries = summaries.copy(deep=True)\n",
    "    sampled_summaries[\"table_copy\"] = sampled_summaries[\"table\"]\n",
    "    sampled_summaries = sampled_summaries.groupby('table_copy', group_keys=False).apply(lambda x: x.sample(n=sample_sizes[x.name], random_state=42), include_groups=False)\n",
    "    return sampled_summaries.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ingest_contents(get_sample_summaries(summaries, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28517/2399945066.py:2: DeprecationWarning: Passing transport options in the API method is deprecated. Use 'Elasticsearch.options()' instead.\n",
      "  client.indices.delete(index='benchmark', ignore=[400, 404])  # Ignore if already removed\n"
     ]
    }
   ],
   "source": [
    "# This is an example for content benchmark\n",
    "ingest_contexts(benchmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Keyword Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tables_ranks(hits):\n",
    "    # Get the tables and the corresponding ranks (based on their relevance scores).\n",
    "    # E.g., [(table_1, 1), (table_2, 1), (table_3, 2), ...]\n",
    "    rank = 1\n",
    "    prev_score = hits[0]['_score']\n",
    "    tables_ranks = []\n",
    "    for hit in hits:\n",
    "        if hit['_score'] < prev_score:\n",
    "            rank += 1\n",
    "        tables_ranks.append((hit['_source']['table'], rank))\n",
    "        prev_score = hit['_score']\n",
    "    return tables_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def evaluate_contexts(benchmark: pd.DataFrame, k=1):\n",
    "    accuracy_sum = 0\n",
    "    precision_at_1_sum = 0\n",
    "    reciprocal_rank_sum = 0\n",
    "    for i in range(benchmark.shape[0]):\n",
    "        expected_tables = ast.literal_eval(benchmark[\"table\"][i])\n",
    "        question = benchmark[\"question\"][i]\n",
    "        search_query = {\n",
    "            \"size\": k,\n",
    "            \"query\": {\n",
    "                \"match\": {\n",
    "                    \"context\": question\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        result = client.search(index=\"benchmark\", body=search_query).body\n",
    "        tables_ranks = get_tables_ranks(result[\"hits\"][\"hits\"])\n",
    "\n",
    "        # Measure the performance\n",
    "        # tables_ranks is the sorted list of tables and ranks retrieved by system.\n",
    "        for j, (table, rank) in enumerate(tables_ranks):\n",
    "            if table in expected_tables:\n",
    "                accuracy_sum += 1\n",
    "                if rank == 1:\n",
    "                    precision_at_1_sum += 1\n",
    "                reciprocal_rank_sum += (1 / (j + 1))\n",
    "                break\n",
    "    return {\n",
    "        \"accuracy\": accuracy_sum / benchmark.shape[0],\n",
    "        \"Mean Precision@1\": precision_at_1_sum / benchmark.shape[0],\n",
    "        \"MRR\": reciprocal_rank_sum / benchmark.shape[0],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_contents(benchmark: pd.DataFrame, k=1):\n",
    "    accuracy_sum = 0\n",
    "    precision_at_1_sum = 0\n",
    "    reciprocal_rank_sum = 0\n",
    "    for i in range(benchmark.shape[0]):\n",
    "        expected_table = benchmark[\"table\"][i]\n",
    "        question = benchmark[\"question\"][i]\n",
    "        search_query = {\n",
    "            \"size\": k,\n",
    "            \"query\": {\n",
    "                \"match\": {\n",
    "                    \"summary\": question\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        result = client.search(index=\"benchmark\", body=search_query).body\n",
    "        tables_ranks = get_tables_ranks(result[\"hits\"][\"hits\"])\n",
    "\n",
    "        # Measure the performance\n",
    "        # tables_ranks is the sorted list of tables and ranks retrieved by system.\n",
    "        for j, (table, rank) in enumerate(tables_ranks):\n",
    "            if table == expected_table:\n",
    "                accuracy_sum += 1\n",
    "                if rank == 1:\n",
    "                    precision_at_1_sum += 1\n",
    "                reciprocal_rank_sum += (1 / (j + 1))\n",
    "                break\n",
    "    return {\n",
    "        \"accuracy\": accuracy_sum / benchmark.shape[0],\n",
    "        \"Mean Precision@1\": precision_at_1_sum / benchmark.shape[0],\n",
    "        \"MRR\": reciprocal_rank_sum / benchmark.shape[0],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.843394323584498, 'Mean Precision@1': 0.7118570811122317, 'MRR': 0.7688133314123798}\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_contexts(benchmark, k=10)  # Adjust k\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
