{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword Search (Weighted by BM25)\n",
    "\n",
    "> Pre-requisite: follow [this guide](https://www.elastic.co/guide/en/elasticsearch/reference/current/getting-started.html) to setup self-managed Elasticsearch system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Connect to Elasticsearch Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "client = Elasticsearch(\n",
    "  \"https://localhost:9200\",\n",
    "  api_key=\"\",  # Adjust\n",
    "  ca_certs=\"http_ca.crt\"\n",
    ")\n",
    "client.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust, this is an example for summaries.\n",
    "# For context benchmark, no need for summaries.\n",
    "import pandas as pd\n",
    "benchmark = pd.read_csv(\"benchmarks/BC2_chicago.csv\")\n",
    "summaries = pd.read_csv(\"benchmarks/row_summaries_chicago.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest Contexts/Contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_contexts(benchmark: pd.DataFrame):\n",
    "    client.indices.delete(index='benchmark', ignore=[400, 404])  # Ignore if already removed\n",
    "    for i in range(benchmark.shape[0]):\n",
    "        table = benchmark[\"table\"][i]\n",
    "        context = benchmark[\"context\"][i]\n",
    "        client.index(\n",
    "            index = \"benchmark\",\n",
    "            document={\n",
    "                \"table\": table,\n",
    "                \"context\": context\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_contents(summaries: pd.DataFrame):\n",
    "    client.indices.delete(index='benchmark', ignore=[400, 404])  # Ignore if already removed\n",
    "    for i in range(summaries.shape[0]):\n",
    "        table = summaries[\"table\"][i]\n",
    "        summary = summaries[\"summary\"][i]\n",
    "        client.index(\n",
    "            index = \"benchmark\",\n",
    "            document={\n",
    "                \"table\": table,\n",
    "                \"summary\": summary\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_sample_summaries(summaries: pd.DataFrame, sample_percentage=1):\n",
    "    \"\"\"\n",
    "    This is to get certain percentage of the summaries.\n",
    "    \"\"\"\n",
    "    # Prepare to sample summaries (category refers to the tables)\n",
    "    category_counts = summaries['table'].value_counts()\n",
    "    sample_sizes = np.ceil(category_counts * sample_percentage).astype(int)\n",
    "\n",
    "    # Perform stratified sampling\n",
    "    sampled_summaries = summaries.copy(deep=True)\n",
    "    sampled_summaries[\"table_copy\"] = sampled_summaries[\"table\"]\n",
    "    sampled_summaries = sampled_summaries.groupby('table_copy', group_keys=False).apply(lambda x: x.sample(n=sample_sizes[x.name], random_state=42), include_groups=False)\n",
    "    return sampled_summaries.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an example for content benchmark\n",
    "ingest_contents(summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Keyword Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tables_ranks(hits):\n",
    "    # Get the tables and the corresponding ranks (based on their relevance scores).\n",
    "    # E.g., [(table_1, 1), (table_2, 1), (table_3, 2), ...]\n",
    "    rank = 1\n",
    "    prev_score = hits[0]['_score']\n",
    "    tables_ranks = []\n",
    "    for hit in hits:\n",
    "        if hit['_score'] < prev_score:\n",
    "            rank += 1\n",
    "        tables_ranks.append((hit['_source']['table'], rank))\n",
    "        prev_score = hit['_score']\n",
    "    return tables_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_contexts(benchmark: pd.DataFrame, k=1):\n",
    "    accuracy_sum = 0\n",
    "    precision_at_1_sum = 0\n",
    "    reciprocal_rank_sum = 0\n",
    "    for i in range(benchmark.shape[0]):\n",
    "        expected_table = benchmark[\"table\"][i]\n",
    "        question = benchmark[\"question\"][i]\n",
    "        search_query = {\n",
    "            \"size\": k,\n",
    "            \"query\": {\n",
    "                \"match\": {\n",
    "                    \"context\": question\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        result = client.search(index=\"benchmark\", body=search_query).body\n",
    "        tables_ranks = get_tables_ranks(result[\"hits\"][\"hits\"])\n",
    "\n",
    "        # Measure the performance\n",
    "        # tables_ranks is the sorted list of tables and ranks retrieved by system.\n",
    "        for j, (table, rank) in enumerate(tables_ranks):\n",
    "            if table == expected_table:\n",
    "                accuracy_sum += 1\n",
    "                if rank == 1:\n",
    "                    precision_at_1_sum += 1\n",
    "                reciprocal_rank_sum += (1 / (j + 1))\n",
    "                break\n",
    "    return {\n",
    "        \"accuracy\": accuracy_sum / benchmark.shape[0],\n",
    "        \"Mean Precision@1\": precision_at_1_sum / benchmark.shape[0],\n",
    "        \"MRR\": reciprocal_rank_sum / benchmark.shape[0],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_contents(benchmark: pd.DataFrame, k=1):\n",
    "    accuracy_sum = 0\n",
    "    precision_at_1_sum = 0\n",
    "    reciprocal_rank_sum = 0\n",
    "    for i in range(benchmark.shape[0]):\n",
    "        expected_table = benchmark[\"table\"][i]\n",
    "        question = benchmark[\"question\"][i]\n",
    "        search_query = {\n",
    "            \"size\": k,\n",
    "            \"query\": {\n",
    "                \"match\": {\n",
    "                    \"summary\": question\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        result = client.search(index=\"benchmark\", body=search_query).body\n",
    "        tables_ranks = get_tables_ranks(result[\"hits\"][\"hits\"])\n",
    "\n",
    "        # Measure the performance\n",
    "        # tables_ranks is the sorted list of tables and ranks retrieved by system.\n",
    "        for j, (table, rank) in enumerate(tables_ranks):\n",
    "            if table == expected_table:\n",
    "                accuracy_sum += 1\n",
    "                if rank == 1:\n",
    "                    precision_at_1_sum += 1\n",
    "                reciprocal_rank_sum += (1 / (j + 1))\n",
    "                break\n",
    "    return {\n",
    "        \"accuracy\": accuracy_sum / benchmark.shape[0],\n",
    "        \"Mean Precision@1\": precision_at_1_sum / benchmark.shape[0],\n",
    "        \"MRR\": reciprocal_rank_sum / benchmark.shape[0],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate_contents(benchmark, k=1)  # Adjust k\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
