{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword Search (Weighted by BM25)\n",
    "\n",
    "> Pre-requisite: follow [this guide](https://www.elastic.co/guide/en/elasticsearch/reference/current/getting-started.html) to setup self-hosted Elasticsearch system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Connect to Elasticsearch Server & Load Benchmarkss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "client = Elasticsearch(\n",
    "  \"https://localhost:9200\",\n",
    "  api_key=\"SEVzTDVvOEJpRjU2U19wVFhrUXM6a204U3lERi1UTUtRNzlQRF9NUENqdw==\",  # Adjust\n",
    "  ca_certs=\"http_ca.crt\"\n",
    ")\n",
    "client.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load context benchmark\n",
    "benchmark = pd.read_csv(\"BX2_public_bi.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load content benchmark + table summaries\n",
    "# benchmark = pd.read_csv(\"BC2_public_bi.csv\")\n",
    "# summaries = pd.read_csv(\"row_summaries_public_bi.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ingest Contexts/Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_contexts(benchmark: pd.DataFrame):\n",
    "    client.indices.delete(index='benchmark', ignore=[400, 404])  # Ignore if already removed\n",
    "    for i in range(benchmark.shape[0]):\n",
    "        table = benchmark[\"table\"][i]\n",
    "        context = benchmark[\"context\"][i]\n",
    "        client.index(\n",
    "            index = \"benchmark\",\n",
    "            document={\n",
    "                \"table\": table,\n",
    "                \"context\": context\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_contents(summaries: pd.DataFrame):\n",
    "    client.indices.delete(index='benchmark', ignore=[400, 404])  # Ignore if already removed\n",
    "    for i in range(summaries.shape[0]):\n",
    "        table = summaries[\"table\"][i]\n",
    "        summary = summaries[\"summary\"][i]\n",
    "        client.index(\n",
    "            index = \"benchmark\",\n",
    "            document={\n",
    "                \"table\": table,\n",
    "                \"summary\": summary\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_sample_summaries(summaries: pd.DataFrame, sample_percentage=1):\n",
    "    \"\"\"\n",
    "    This is to randomly sample certain percentage of the summaries.\n",
    "    The return value is the summaries but only for the sampled rows.\n",
    "    \"\"\"\n",
    "    # Prepare to sample summaries (category refers to the tables)\n",
    "    category_counts = summaries[\"table\"].value_counts()\n",
    "    sample_sizes = np.ceil(category_counts * sample_percentage).astype(int)\n",
    "\n",
    "    # Perform stratified sampling\n",
    "    sampled_summaries = summaries.copy(deep=True)\n",
    "    sampled_summaries[\"table_copy\"] = sampled_summaries[\"table\"]\n",
    "    sampled_summaries = sampled_summaries.groupby(\"table_copy\", group_keys=False).apply(\n",
    "        lambda x: x.sample(n=sample_sizes[x.name], random_state=42),\n",
    "        include_groups=False,\n",
    "    )\n",
    "    return sampled_summaries.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest contexts\n",
    "ingest_contexts(benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest summaries\n",
    "# ingest_contents(summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_tables_ranks(hits):\n",
    "    # Get the tables and the corresponding ranks (based on their relevance scores).\n",
    "    rank = 1\n",
    "    prev_score = hits[0]['_score']\n",
    "    tables_encountered = defaultdict(bool)\n",
    "    tables_ranks = []\n",
    "    for hit in hits:\n",
    "        table = hit['_source']['table']\n",
    "        if not tables_encountered[table]:\n",
    "            if hit['_score'] < prev_score:\n",
    "                rank += 1\n",
    "            tables_ranks.append((table, rank))\n",
    "            prev_score = hit['_score']\n",
    "            tables_encountered[table] = True\n",
    "    return tables_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def evaluate_contexts(benchmark: pd.DataFrame, k=1):\n",
    "    accuracy_sum = 0\n",
    "    precision_at_1_sum = 0\n",
    "    reciprocal_rank_sum = 0\n",
    "    for i in range(benchmark.shape[0]):\n",
    "        expected_tables = ast.literal_eval(benchmark[\"relevant_tables\"][i])\n",
    "        question = benchmark[\"question\"][i]\n",
    "        search_query = {\n",
    "            \"size\": k,\n",
    "            \"query\": {\n",
    "                \"match\": {\n",
    "                    \"context\": question\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        result = client.search(index=\"benchmark\", body=search_query, search_type='dfs_query_then_fetch').body\n",
    "        tables_ranks = get_tables_ranks(result[\"hits\"][\"hits\"])\n",
    "    \n",
    "        for j, (table, rank) in enumerate(tables_ranks):\n",
    "            if table in expected_tables:\n",
    "                accuracy_sum += 1\n",
    "                if rank == 1:\n",
    "                    precision_at_1_sum += 1\n",
    "                reciprocal_rank_sum += (1 / (j + 1))\n",
    "                break\n",
    "    return {\n",
    "        \"accuracy\": accuracy_sum / benchmark.shape[0],\n",
    "        \"Mean Precision@1\": precision_at_1_sum / benchmark.shape[0],\n",
    "        \"MRR\": reciprocal_rank_sum / benchmark.shape[0],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_contents(benchmark: pd.DataFrame, k=1):\n",
    "    accuracy_sum = 0\n",
    "    precision_at_1_sum = 0\n",
    "    reciprocal_rank_sum = 0\n",
    "    for i in range(benchmark.shape[0]):\n",
    "        expected_table = benchmark[\"table\"][i]\n",
    "        question = benchmark[\"question\"][i]\n",
    "        search_query = {\n",
    "            \"size\": k,\n",
    "            \"query\": {\n",
    "                \"match\": {\n",
    "                    \"summary\": question\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        result = client.search(index=\"benchmark\", body=search_query, search_type='dfs_query_then_fetch').body\n",
    "        tables_ranks = get_tables_ranks(result[\"hits\"][\"hits\"])\n",
    "\n",
    "        for j, (table, rank) in enumerate(tables_ranks):\n",
    "            if table == expected_table:\n",
    "                accuracy_sum += 1\n",
    "                if rank == 1:\n",
    "                    precision_at_1_sum += 1\n",
    "                reciprocal_rank_sum += (1 / (j + 1))\n",
    "                break\n",
    "    return {\n",
    "        \"accuracy\": accuracy_sum / benchmark.shape[0],\n",
    "        \"Mean Precision@1\": precision_at_1_sum / benchmark.shape[0],\n",
    "        \"MRR\": reciprocal_rank_sum / benchmark.shape[0],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate_contexts(benchmark, k=1)  # Adjust k\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
