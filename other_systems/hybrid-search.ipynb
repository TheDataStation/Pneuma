{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Hybrid Search"]},{"cell_type":"markdown","metadata":{},"source":["## 0. Create Custom Retriever\n","\n","> Hybrid Retriever: BM25 + Vector Retriever"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[],"source":["from llama_index.core.postprocessor import SentenceTransformerRerank\n","from llama_index.core import QueryBundle\n","\n","\n","class HybridRetrieverOriginal:\n","    def __init__(self, bm25_retriever, vector_retriever):\n","        self.bm25_retriever = bm25_retriever\n","        self.vector_retriever = vector_retriever\n","        self.reranker = SentenceTransformerRerank(model=\"BAAI/bge-reranker-base\")\n","\n","    def retrieve(self, query, top_k=10):\n","        # 1. Change the top_k of the retrievers & reranker\n","        self.vector_retriever.similarity_top_k = top_k\n","        self.bm25_retriever.similarity_top_k = top_k\n","        self.reranker.top_n = top_k\n","\n","        # 2. Use both retrievers to get top-k results + normalization\n","        bm25_nodes = self.bm25_retriever.retrieve(query)\n","        vector_nodes = self.vector_retriever.retrieve(query)\n","\n","        # 3. Combine the two lists of nodes\n","        all_nodes = []\n","        node_ids = set()\n","        for n in bm25_nodes + vector_nodes:\n","            if n.node.node_id not in node_ids:\n","                all_nodes.append(n)\n","                node_ids.add(n.node.node_id)\n","\n","        # 4. Rerank and get top-k results\n","        reranked_nodes = self.reranker.postprocess_nodes(\n","            all_nodes,\n","            query_bundle=QueryBundle(query),\n","        )\n","\n","        return reranked_nodes"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[],"source":["class HybridRetriever:\n","    def __init__(self, bm25_retriever, vector_retriever, alpha=0.5):\n","        self.bm25_retriever = bm25_retriever\n","        self.vector_retriever = vector_retriever\n","        self.alpha = alpha\n","\n","    def process_nodes(self, nodes):\n","        # Normalize relevance scores and return the nodes in dict format.\n","        scores: list[float] = [node.score for node in nodes]\n","        max_score = max(scores)\n","        min_score = min(scores)\n","        \n","        processed_nodes = {}\n","        for node in nodes:\n","            if min_score == max_score:\n","                node.score = 1\n","            else:\n","                node.score = (node.score - min_score) / (max_score - min_score)\n","            processed_nodes[node.id_] = node\n","        return processed_nodes\n","\n","    def retrieve(self, query, top_k=10):\n","        # 1. Change the top_k of the retrievers\n","        self.vector_retriever.similarity_top_k = top_k\n","        self.bm25_retriever.similarity_top_k = top_k\n","\n","        # 2. Use both retrievers to get top-k results + normalization\n","        bm25_nodes = self.process_nodes(self.bm25_retriever.retrieve(query))\n","        vector_nodes = self.process_nodes(self.vector_retriever.retrieve(query))\n","\n","        # 3. Linearly combine the scores of each node\n","        node_ids = set(list(bm25_nodes.keys()) + list(vector_nodes.keys()))\n","        all_nodes = []\n","        for node_id in node_ids:\n","            try:\n","                bm25_score = bm25_nodes.get(node_id).score\n","            except:\n","                bm25_score = 0.0\n","            try:\n","                cosine_score = vector_nodes.get(node_id).score\n","            except:\n","                cosine_score = 0.0\n","            combined_score = self.alpha * bm25_score + (1 - self.alpha) * cosine_score\n","            node = bm25_nodes.get(node_id, vector_nodes.get(node_id))\n","            node.score = combined_score\n","\n","            all_nodes.append(node)\n","        \n","        sorted_nodes = sorted(all_nodes, key=lambda node: node.score, reverse=True)[:top_k]\n","        return sorted_nodes"]},{"cell_type":"markdown","metadata":{},"source":["## 1. Load Embedding Model"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2024-05-16T14:38:28.847209Z","iopub.status.busy":"2024-05-16T14:38:28.846888Z","iopub.status.idle":"2024-05-16T14:38:38.333823Z","shell.execute_reply":"2024-05-16T14:38:38.332945Z","shell.execute_reply.started":"2024-05-16T14:38:28.847181Z"},"trusted":true},"outputs":[],"source":["from llama_index.core import (\n","    Settings,\n","    VectorStoreIndex,\n","    Document,\n",")\n","\n","from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n","from llama_index.retrievers.bm25 import BM25Retriever\n","\n","import pandas as pd"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2024-05-16T14:38:38.356741Z","iopub.status.busy":"2024-05-16T14:38:38.356437Z","iopub.status.idle":"2024-05-16T14:38:42.957868Z","shell.execute_reply":"2024-05-16T14:38:42.956861Z","shell.execute_reply.started":"2024-05-16T14:38:38.356716Z"},"trusted":true},"outputs":[],"source":["Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[],"source":["benchmark = pd.read_csv(\"bx/BX1_chicago.csv\")\n","# summaries = pd.read_csv(\"bc/row_summaries_public_bi.csv\")"]},{"cell_type":"markdown","metadata":{},"source":["## 2. Index Contexts/Contents"]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2024-05-16T07:30:26.139379Z","iopub.status.busy":"2024-05-16T07:30:26.139035Z","iopub.status.idle":"2024-05-16T07:30:26.145184Z","shell.execute_reply":"2024-05-16T07:30:26.144209Z","shell.execute_reply.started":"2024-05-16T07:30:26.139354Z"},"trusted":true},"outputs":[],"source":["def create_context_documents(df):\n","    documents = []\n","    for idx in df.index:\n","        table = df[\"table\"][idx]\n","        answer = df[\"context\"][idx]\n","        document = Document(\n","            text=answer,\n","            metadata={\"table\": table},\n","            doc_id=f\"doc_'{table}'_{idx}\",\n","        )\n","        documents.append(document)\n","    return documents"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[],"source":["def create_content_documents(df):\n","    documents = []\n","    for idx in df.index:\n","        table = df[\"table\"][idx]\n","        table_summary = df[\"summary\"][idx]\n","        document = Document(\n","            text=table_summary,\n","            metadata={\"table\": table},\n","            doc_id=f\"doc_'{table}'_{idx}\",\n","        )\n","        documents.append(document)\n","    return documents"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[],"source":["import numpy as np\n","\n","def get_sample_summaries(summaries: pd.DataFrame, sample_percentage=1):\n","    \"\"\"\n","    This is to randomly sample certain percentage of the summaries.\n","    The return value is the df itself, but only the sampled rows remained.\n","    \"\"\"\n","    # Prepare to sample summaries (category refers to the tables)\n","    category_counts = summaries[\"table\"].value_counts()\n","    sample_sizes = np.ceil(category_counts * sample_percentage).astype(int)\n","\n","    # Perform stratified sampling\n","    sampled_summaries = summaries.copy(deep=True)\n","    sampled_summaries[\"table_copy\"] = sampled_summaries[\"table\"]\n","    sampled_summaries = sampled_summaries.groupby(\"table_copy\", group_keys=False).apply(\n","        lambda x: x.sample(n=sample_sizes[x.name], random_state=42),\n","        include_groups=False,\n","    )\n","    return sampled_summaries.reset_index(drop=True)"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[{"data":{"text/plain":["1356"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["# For example, this is for context\n","documents = create_context_documents(benchmark)\n","len(documents)"]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[],"source":["# documents = create_content_documents(summaries)"]},{"cell_type":"code","execution_count":48,"metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2024-05-16T07:30:26.987299Z","iopub.status.busy":"2024-05-16T07:30:26.987015Z","iopub.status.idle":"2024-05-16T07:32:34.079446Z","shell.execute_reply":"2024-05-16T07:32:34.078552Z","shell.execute_reply.started":"2024-05-16T07:30:26.987274Z"},"scrolled":true,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Index created\n"]}],"source":["vector_index = VectorStoreIndex(documents)\n","print(\"Index created\")"]},{"cell_type":"markdown","metadata":{},"source":["## 3. Evaluate Vector Search"]},{"cell_type":"code","execution_count":49,"metadata":{"execution":{"iopub.execute_input":"2024-05-16T07:32:34.082528Z","iopub.status.busy":"2024-05-16T07:32:34.082250Z","iopub.status.idle":"2024-05-16T07:32:34.088374Z","shell.execute_reply":"2024-05-16T07:32:34.087473Z","shell.execute_reply.started":"2024-05-16T07:32:34.082503Z"},"trusted":true},"outputs":[],"source":["def convert_retrieved_data_to_tables_ranks(retrieved_data):\n","    # Convert all retrieved data to the format (table, rank)\n","    rank = 1\n","    prev_score = retrieved_data[0].get_score()\n","    tables_ranks = []\n","    for data in retrieved_data:\n","        if data.get_score() < prev_score:\n","            rank += 1\n","        table = data.id_.split(\"'\")[1]  # E.g., \"chicago_open_data/22u3-xenr\"\n","        tables_ranks.append((table, rank))\n","        prev_score = data.get_score()\n","    return tables_ranks"]},{"cell_type":"code","execution_count":50,"metadata":{"execution":{"iopub.execute_input":"2024-05-16T07:32:34.089960Z","iopub.status.busy":"2024-05-16T07:32:34.089545Z","iopub.status.idle":"2024-05-16T07:32:34.097983Z","shell.execute_reply":"2024-05-16T07:32:34.097144Z","shell.execute_reply.started":"2024-05-16T07:32:34.089934Z"},"trusted":true},"outputs":[],"source":["def evaluate(retriever, benchmark_df, top_k=1):\n","    accuracy_sum = 0\n","    precision_at_1_sum = 0\n","    reciprocal_rank_sum = 0\n","    for i in range(len(benchmark_df)):\n","        query = benchmark_df[\"question\"][i]\n","        expected_table = benchmark_df[\"table\"][i]\n","        retrieved_data = retriever.retrieve(query, top_k)\n","        tables_ranks = convert_retrieved_data_to_tables_ranks(retrieved_data)\n","        for j, (table, rank) in enumerate(tables_ranks):\n","            if table == expected_table:\n","                accuracy_sum += 1\n","                if rank == 1:\n","                    precision_at_1_sum += 1\n","                reciprocal_rank_sum += 1 / (j + 1)\n","                break\n","        if i % 100 == 0:\n","            print(i)\n","            print(\"Accuracy:\", accuracy_sum)\n","            print(\"Prec@1:\", precision_at_1_sum)\n","            print(\"Reciprocal Rank:\", reciprocal_rank_sum)\n","    return {\n","        \"accuracy\": accuracy_sum / benchmark_df.shape[0],\n","        \"Mean Precision@1\": precision_at_1_sum / benchmark_df.shape[0],\n","        \"MRR\": reciprocal_rank_sum / benchmark_df.shape[0],\n","    }"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0\n","Accuracy: 1\n","Prec@1: 1\n","Reciprocal Rank: 1.0\n","100\n","Accuracy: 95\n","Prec@1: 88\n","Reciprocal Rank: 83.76666666666668\n","200\n","Accuracy: 192\n","Prec@1: 184\n","Reciprocal Rank: 176.76666666666668\n","300\n","Accuracy: 286\n","Prec@1: 274\n","Reciprocal Rank: 267.75\n","400\n","Accuracy: 383\n","Prec@1: 365\n","Reciprocal Rank: 358.66666666666663\n","500\n","Accuracy: 478\n","Prec@1: 456\n","Reciprocal Rank: 448.91666666666663\n","600\n","Accuracy: 568\n","Prec@1: 542\n","Reciprocal Rank: 533.0333333333333\n","700\n","Accuracy: 661\n","Prec@1: 630\n","Reciprocal Rank: 619.2333333333335\n","800\n","Accuracy: 759\n","Prec@1: 727\n","Reciprocal Rank: 714.2333333333335\n","900\n","Accuracy: 855\n","Prec@1: 820\n","Reciprocal Rank: 806.6000000000003\n","1000\n","Accuracy: 953\n","Prec@1: 913\n","Reciprocal Rank: 899.7833333333338\n","1100\n","Accuracy: 1050\n","Prec@1: 1005\n","Reciprocal Rank: 992.7833333333339\n","1200\n","Accuracy: 1145\n","Prec@1: 1093\n","Reciprocal Rank: 1083.316666666667\n","1300\n","Accuracy: 1240\n","Prec@1: 1183\n","Reciprocal Rank: 1173.9833333333336\n","{'accuracy': 0.9542772861356932, 'Mean Precision@1': 0.9085545722713865, 'MRR': 0.9026671583087514}\n"]}],"source":["vector_retriever = vector_index.as_retriever()\n","BM25_retriever = BM25Retriever.from_defaults(vector_index)\n","hybrid_retriever = HybridRetriever(BM25_retriever, vector_retriever)\n","result = evaluate(hybrid_retriever, benchmark, top_k=5)\n","print(result)"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0\n","Accuracy: 1\n","Prec@1: 1\n","Reciprocal Rank: 1.0\n","100\n","Accuracy: 99\n","Prec@1: 86\n","Reciprocal Rank: 84.09007936507938\n","200\n","Accuracy: 197\n","Prec@1: 180\n","Reciprocal Rank: 178.75674603174605\n","300\n","Accuracy: 295\n","Prec@1: 270\n","Reciprocal Rank: 270.2757936507936\n","400\n","Accuracy: 392\n","Prec@1: 358\n","Reciprocal Rank: 361.02579365079356\n","500\n","Accuracy: 489\n","Prec@1: 449\n","Reciprocal Rank: 452.56746031746025\n","600\n","Accuracy: 583\n","Prec@1: 534\n","Reciprocal Rank: 539.2297619047617\n","700\n","Accuracy: 679\n","Prec@1: 622\n","Reciprocal Rank: 627.3642857142856\n","800\n","Accuracy: 779\n","Prec@1: 718\n","Reciprocal Rank: 722.6003968253967\n","900\n","Accuracy: 877\n","Prec@1: 810\n","Reciprocal Rank: 815.7210317460317\n","1000\n","Accuracy: 975\n","Prec@1: 902\n","Reciprocal Rank: 908.8043650793652\n","1100\n","Accuracy: 1075\n","Prec@1: 993\n","Reciprocal Rank: 1002.196031746032\n","1200\n","Accuracy: 1174\n","Prec@1: 1081\n","Reciprocal Rank: 1093.2\n","1300\n","Accuracy: 1271\n","Prec@1: 1170\n","Reciprocal Rank: 1184.1583333333333\n","{'accuracy': 0.9778761061946902, 'Mean Precision@1': 0.8989675516224189, 'MRR': 0.9106624877089479}\n"]}],"source":["vector_retriever = vector_index.as_retriever()\n","BM25_retriever = BM25Retriever.from_defaults(vector_index)\n","hybrid_retriever = HybridRetriever(BM25_retriever, vector_retriever)\n","result = evaluate(hybrid_retriever, benchmark, top_k=10)\n","print(result)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4995818,"sourceId":8426657,"sourceType":"datasetVersion"},{"datasetId":5021434,"sourceId":8431643,"sourceType":"datasetVersion"}],"dockerImageVersionId":30665,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":4}
