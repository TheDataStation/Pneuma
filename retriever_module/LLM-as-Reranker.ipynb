{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Search + LLM\n",
    "\n",
    "> LLM as filterer of rankings provided by hybrid retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to choose GPU if needed\n",
    "# import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "# import setproctitle\n",
    "# setproctitle.setproctitle(\"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import (\n",
    "    Settings,\n",
    "    VectorStoreIndex,\n",
    "    Document,\n",
    ")\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# These packages are available in the benchmark_generator/context directory\n",
    "from utils.pipeline_initializer import initialize_pipeline\n",
    "from utils.prompting_interface import prompt_pipeline\n",
    "\n",
    "import torch\n",
    "import warnings\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize Models & Index Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "pipe = initialize_pipeline(\"mistralai/Mistral-7B-Instruct-v0.3\", torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark = pd.read_csv(\"BX1_chicago_corrected.csv\")  # Adjust name\n",
    "# summaries = pd.read_csv(\"summaries_public_bi.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_content_documents(df):\n",
    "    documents = []\n",
    "    for idx in df.index:\n",
    "        table = df[\"table\"][idx]\n",
    "        table_summary = df[\"summary\"][idx]\n",
    "        document = Document(\n",
    "            text=table_summary,\n",
    "            metadata={\"table\": table},\n",
    "            doc_id=f\"doc_'{table}'_{idx}\",\n",
    "        )\n",
    "        documents.append(document)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_context_documents(df):\n",
    "    documents = []\n",
    "    for idx in df.index:\n",
    "        table = df[\"table\"][idx]\n",
    "        answer = df[\"context\"][idx]\n",
    "        document = Document(\n",
    "            text=answer,\n",
    "            metadata={\"table\": table},\n",
    "            doc_id=f\"doc_'{table}'_{idx}\",\n",
    "        )\n",
    "        documents.append(document)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = create_context_documents(benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index = VectorStoreIndex(documents)\n",
    "print(\"Index created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridRetriever:\n",
    "    def __init__(self, bm25_retriever, vector_retriever, alpha=0.5):\n",
    "        self.bm25_retriever = bm25_retriever\n",
    "        self.vector_retriever = vector_retriever\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def process_nodes(self, nodes):\n",
    "        # Normalize relevance scores and return the nodes in dict format.\n",
    "        scores: list[float] = [node.score for node in nodes]\n",
    "        max_score = max(scores)\n",
    "        min_score = min(scores)\n",
    "        \n",
    "        processed_nodes = {}\n",
    "        for node in nodes:\n",
    "            if min_score == max_score:\n",
    "                node.score = 1\n",
    "            else:\n",
    "                node.score = (node.score - min_score) / (max_score - min_score)\n",
    "            processed_nodes[node.id_] = node\n",
    "        return processed_nodes\n",
    "\n",
    "    def retrieve(self, query, top_k=10):\n",
    "        # 1. Change the top_k of the retrievers\n",
    "        self.vector_retriever.similarity_top_k = top_k\n",
    "        self.bm25_retriever.similarity_top_k = top_k\n",
    "\n",
    "        # 2. Use both retrievers to get top-k results + normalization\n",
    "        bm25_nodes = self.process_nodes(self.bm25_retriever.retrieve(query))\n",
    "        vector_nodes = self.process_nodes(self.vector_retriever.retrieve(query))\n",
    "\n",
    "        # 3. Linearly combine the scores of each node\n",
    "        node_ids = set(list(bm25_nodes.keys()) + list(vector_nodes.keys()))\n",
    "        all_nodes = []\n",
    "        for node_id in node_ids:\n",
    "            try:\n",
    "                bm25_score = bm25_nodes.get(node_id).score\n",
    "            except:\n",
    "                bm25_score = 0.0\n",
    "            try:\n",
    "                cosine_score = vector_nodes.get(node_id).score\n",
    "            except:\n",
    "                cosine_score = 0.0\n",
    "            combined_score = self.alpha * bm25_score + (1 - self.alpha) * cosine_score\n",
    "            node = bm25_nodes.get(node_id, vector_nodes.get(node_id))\n",
    "            node.score = combined_score\n",
    "\n",
    "            all_nodes.append(node)\n",
    "        \n",
    "        sorted_nodes = sorted(all_nodes, key=lambda node: node.score, reverse=True)[:top_k]\n",
    "        return sorted_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevancy_prompt(metadata: str, query: str):\n",
    "    return f\"\"\"Metadata M:\"{metadata}\"\n",
    "Query Q: \"{query}\"\n",
    "Metadata M is associated with a dataset that we can access. Is this dataset relevant to query Q? Begin your argument with yes/no.\"\"\"\n",
    "\n",
    "def filter_retrieved_data(retrieved_data, query):\n",
    "    filtered_data = []\n",
    "    for data in retrieved_data:\n",
    "        metadata = data.text\n",
    "        prompt = get_relevancy_prompt(metadata, query)\n",
    "        conversation = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        model_output: str = prompt_pipeline(pipe, conversation, max_new_tokens=4, context_length=32768)[-1][\n",
    "            \"content\"\n",
    "        ]\n",
    "        # print(f\"DEBUG prompt: {prompt}\")\n",
    "        # print(\"=\" * 100)\n",
    "        # print(f\"DEBUG model output: {model_output}\")\n",
    "        # print(\"=\" * 100)\n",
    "        if model_output.lower().strip().startswith(\"yes\") or model_output.lower().strip().startswith(\"**yes**\"):\n",
    "            filtered_data.append((data, True))\n",
    "        else:\n",
    "            filtered_data.append((data, False))\n",
    "    filtered_data.sort(key=lambda x: not x[1])\n",
    "    filtered_data = [x[0] for x in filtered_data]\n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_retrieved_data_to_tables_ranks(retrieved_data, query):\n",
    "    # Convert all retrieved data to the format (table, rank)\n",
    "    rank = 1\n",
    "    filtered_data = filter_retrieved_data(retrieved_data, query)  # Filter with LLM\n",
    "    prev_score = -1000\n",
    "    tables_ranks = []\n",
    "    for data in filtered_data:\n",
    "        if data.get_score() < prev_score:\n",
    "            rank += 1\n",
    "        table = data.id_.split(\"'\")[1]  # E.g., \"chicago_open_data/22u3-xenr\"\n",
    "        tables_ranks.append((table, rank))\n",
    "        prev_score = data.get_score()\n",
    "    return tables_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def evaluate(retriever, benchmark_df, top_k=1):\n",
    "    accuracy_sum = 0\n",
    "    precision_at_1_sum = 0\n",
    "    reciprocal_rank_sum = 0\n",
    "    for i in range(len(benchmark_df)):\n",
    "        query = benchmark_df[\"question\"][i]\n",
    "        try:\n",
    "            expected_tables = ast.literal_eval(benchmark_df[\"relevant_tables\"][i])\n",
    "        except:\n",
    "            expected_tables = [benchmark_df[\"table\"][i]]\n",
    "        retrieved_data = retriever.retrieve(query, top_k)  # Using hybrid retriever\n",
    "        tables_ranks = convert_retrieved_data_to_tables_ranks(retrieved_data, query)\n",
    "\n",
    "        before = accuracy_sum\n",
    "        for j, (table, rank) in enumerate(tables_ranks):\n",
    "            if table in expected_tables:\n",
    "                accuracy_sum += 1\n",
    "                if rank == 1:\n",
    "                    precision_at_1_sum += 1\n",
    "                reciprocal_rank_sum += 1 / (j + 1)\n",
    "                break\n",
    "        if accuracy_sum == before:\n",
    "            print(f\"Wrong answer at index {i}\")\n",
    "\n",
    "        if i % 25 == 0:  # Checkpointing\n",
    "            print(f\"i: {i}\")\n",
    "            print(f\"accuracy_sum: {accuracy_sum}\")\n",
    "            print(f\"precision_at_1_sum: {precision_at_1_sum}\")\n",
    "            print(f\"reciprocal_rank_sum: {reciprocal_rank_sum}\")\n",
    "            print(\"=\" * 100)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_sum / benchmark_df.shape[0],\n",
    "        \"Mean Precision@1\": precision_at_1_sum / benchmark_df.shape[0],\n",
    "        \"MRR\": reciprocal_rank_sum / benchmark_df.shape[0],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "vector_retriever = vector_index.as_retriever()\n",
    "BM25_retriever = BM25Retriever.from_defaults(vector_index)\n",
    "hybrid_retriever = HybridRetriever(BM25_retriever, vector_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = evaluate(hybrid_retriever, benchmark, top_k=1)  # Adjust k\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
