{"cells":[{"cell_type":"markdown","metadata":{},"source":["# RAG (currently for context only)"]},{"cell_type":"markdown","metadata":{},"source":["## 1. Install and Run Ollama Models"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2024-05-24T15:41:53.174779Z","iopub.status.busy":"2024-05-24T15:41:53.174321Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["!curl https://ollama.ai/install.sh | sed 's#https://ollama.ai/download#https://github.com/jmorganca/ollama/releases/download/v0.1.20#' | sh\n","!sudo apt install -y neofetch"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["OLLAMA_MODEL='mistral'\n","\n","# Set it at the OS level\n","import os\n","os.environ['OLLAMA_MODEL'] = OLLAMA_MODEL\n","!echo $OLLAMA_MODEL"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import subprocess\n","import time\n","\n","# Start ollama as a background process\n","command = \"nohup ollama serve&\"\n","\n","# Use subprocess.Popen to start the process in the background\n","process = subprocess.Popen(command,\n","                            shell=True,\n","                           stdout=subprocess.PIPE,\n","                           stderr=subprocess.PIPE)\n","print(\"Process ID:\", process.pid)\n","# Let's use fly.io resources\n","#!OLLAMA_HOST=https://ollama-demo.fly.dev:443\n","time.sleep(5)  # Makes Python wait for 5 seconds"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!ollama -v"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"scrolled":true,"trusted":true},"outputs":[],"source":["!ollama run $OLLAMA_MODEL \"Translate the following emoji sentence to a meaningful text : ðŸ’™ðŸ¤“ðŸ’»ðŸ“±ðŸ”§ðŸŒ€ðŸ’¡\""]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"scrolled":true,"trusted":true},"outputs":[],"source":["!pip install llama-index-core\n","!pip install llama-index-llms-ollama\n","!pip install llama-index-embeddings-huggingface"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from llama_index.core import (\n","    Settings,\n","    VectorStoreIndex,\n","    Document,\n",")\n","\n","from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n","from llama_index.llms.ollama import Ollama"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["Settings.embed_model = HuggingFaceEmbedding(\n","    model_name=\"BAAI/bge-small-en-v1.5\"\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["Settings.llm = Ollama(\n","    model=\"mistral\", request_timeout=1000\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["bx1_df = pd.read_csv('/kaggle/input/pneuma-benchmarks/chicago_open_data/BX1_chicago.csv')\n","bx1_df.head()"]},{"cell_type":"markdown","metadata":{},"source":["# 2. Index Documents"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def create_documents(df):\n","    documents = []\n","    for idx in df.index:\n","        table = df['table'][idx]\n","        answer = df['context'][idx]\n","        document = Document(\n","                text=answer,\n","                metadata={\n","                    'table': table\n","                },\n","                doc_id=f\"doc_{idx}\",\n","            )\n","        documents.append(document)        \n","    return documents"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["documents = create_documents(bx1_df)\n","len(documents)"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"scrolled":true,"trusted":true},"outputs":[],"source":["vector_index = VectorStoreIndex(documents);\n","print(\"Index created\")"]},{"cell_type":"markdown","metadata":{},"source":["# 3. Evaluate RAG"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def get_query(question: str):\n","    return f\"\"\"{question} Provide your response in the following format:\n","- Datasets: [The dataset(s) that are relevant to the query, ordered from the most relevant]\n","- Explanation: [Explain why these dataset(s) are relevant to the query]\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def evaluate_benchmark(benchmark, query_engine):\n","    accuracy_sum = 0\n","    precision_at_1_sum = 0\n","    reciprocal_rank_sum = 0\n","    for i in range(len(benchmark)):\n","        question = benchmark[\"question\"][i]\n","        expected_dataset = benchmark[\"table\"][i]\n","        query = get_query(question)\n","        query_response = query_engine.query(query)\n","        datasets_line = query_response.response.split(\"\\n\")[0].strip()\n","        datasets = datasets_line.split(': ')[1].split(', ')\n","        for rank, dataset in enumerate(datasets):\n","            if dataset == expected_dataset:\n","                accuracy_sum += 1\n","                if rank == 0:\n","                    precision_at_1_sum += 1\n","                reciprocal_rank_sum += (1 / (rank + 1))\n","                break\n","        if i % 25 == 0:  # Checkpointing\n","            print(\"=\" * 50)\n","            print(f\"Index: {i}\")\n","            print(accuracy_sum)\n","            print(precision_at_1_sum)\n","            print(reciprocal_rank_sum)\n","            print(\"=\" * 50)\n","    return {\n","        \"accuracy\": accuracy_sum/len(benchmark),\n","        \"Mean Precision@1\": precision_at_1_sum/len(benchmark),\n","        \"MRR\": reciprocal_rank_sum/len(benchmark),\n","    }"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import datetime\n","start_time = datetime.datetime.now()\n","result = evaluate_benchmark(\n","    bx1_df.iloc[:,:].reset_index(drop=True),  # Adjust how many rows to process\n","    vector_index.as_query_engine(similarity_top_k=10)  # Adjust k\n",")\n","end_time = datetime.datetime.now()\n","time_diff = end_time - start_time\n","total_seconds = time_diff.total_seconds()\n","\n","print(f\"Total time elapsed: {total_seconds} seconds\")\n","print(f\"Result: {result}\")"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4815872,"sourceId":8162898,"sourceType":"datasetVersion"},{"datasetId":4992603,"sourceId":8392827,"sourceType":"datasetVersion"},{"datasetId":4995818,"sourceId":8426657,"sourceType":"datasetVersion"}],"dockerImageVersionId":30665,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
