{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index-core\n",
    "!pip install llama-index-llms-ollama\n",
    "!pip install llama-index-embeddings-huggingface\n",
    "!pip install pandas\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl https://ollama.ai/install.sh | sed 's#https://ollama.ai/download#https://github.com/jmorganca/ollama/releases/download/v0.1.20#' | sh\n",
    "!sudo apt install -y neofetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!neofetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_MODEL='mistral'\n",
    "\n",
    "# Set it at the OS level\n",
    "import os\n",
    "os.environ['OLLAMA_MODEL'] = OLLAMA_MODEL\n",
    "!echo $OLLAMA_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "# Start ollama as a background process\n",
    "command = \"nohup ollama serve&\"\n",
    "\n",
    "# Use subprocess.Popen to start the process in the background\n",
    "process = subprocess.Popen(command,\n",
    "                            shell=True,\n",
    "                           stdout=subprocess.PIPE,\n",
    "                           stderr=subprocess.PIPE)\n",
    "print(\"Process ID:\", process.pid)\n",
    "# Let's use fly.io resources\n",
    "#!OLLAMA_HOST=https://ollama-demo.fly.dev:443\n",
    "time.sleep(5)  # Makes Python wait for 5 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama run $OLLAMA_MODEL \"Translate the following emoji sentence to a meaningful text : ðŸ’™ðŸ¤“ðŸ’»ðŸ“±ðŸ”§ðŸŒ€ðŸ’¡\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import (\n",
    "    Settings,\n",
    "    VectorStoreIndex,\n",
    "    Document,\n",
    ")\n",
    "\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "from llama_index.core.evaluation import RetrieverEvaluator\n",
    "\n",
    "from llama_index.core.evaluation import (\n",
    "    EmbeddingQAFinetuneDataset,\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
    ")\n",
    "Settings.llm = Ollama(\n",
    "    model=\"mistral\", request_timeout=60.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('benchmark.csv')\n",
    "df = df.drop(df[df['E'] == 'bad'].index)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in set(df['T']):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Each Row as a Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "eval_dataset = {\n",
    "    'queries': dict(),\n",
    "    'corpus': dict(),\n",
    "    'relevant_docs': dict()\n",
    "}\n",
    "\n",
    "for idx in df.index:\n",
    "    table = df['T'][idx]\n",
    "    question = df['Q'][idx]\n",
    "    answer = df['A'][idx]\n",
    "    document = Document(\n",
    "            text=answer,\n",
    "            metadata={\n",
    "                'table': table\n",
    "            },\n",
    "            doc_id=f\"doc_{idx}\",\n",
    "        )\n",
    "    documents.append(document)\n",
    "    \n",
    "    eval_dataset['queries'].update(\n",
    "        {f\"query_{idx}\": \n",
    "             f\"For {table}, {question}\"}\n",
    "    )\n",
    "    eval_dataset['corpus'].update(\n",
    "        {f\"doc_{idx}\": answer}\n",
    "    )\n",
    "    eval_dataset['relevant_docs'].update(\n",
    "        {f\"query_{idx}\": [f\"doc_{idx}\"]}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eval_dataset['queries']['query_0'])\n",
    "print()\n",
    "print(eval_dataset['corpus']['doc_0'])\n",
    "print()\n",
    "print(eval_dataset['relevant_docs']['query_0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index = VectorStoreIndex(documents)\n",
    "print(\"Index created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"For what purpose was the \n",
    "TrainsUK1 dataset created? Was there a specific task in mind? \n",
    "Was there a specific gap that needed to be filled? Please provide a description.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_index.as_retriever(similarity_top_k=4)\n",
    "retrieved_data = retriever.retrieve(query)\n",
    "for data in retrieved_data:\n",
    "    display_source_node(data, source_length=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"Which dataset was designed to analyze the punctuality and performance?\n",
    "Mention the name of the dataset.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = vector_index.as_query_engine()\n",
    "query_response = query_engine.query(query)\n",
    "print(\"Response:\\n\", query_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retriever top-K Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The free API is limited to 10 calls per minute\n",
    "include_cohere_rerank = False\n",
    "\n",
    "if include_cohere_rerank:\n",
    "    !pip install cohere -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"mrr\", \"hit_rate\"]\n",
    "retriever_names = []\n",
    "evaluators = []\n",
    "\n",
    "if include_cohere_rerank:\n",
    "    metrics.append(\n",
    "        \"cohere_rerank_relevancy\"  # requires COHERE_API_KEY environment variable to be set\n",
    "    )\n",
    "\n",
    "for k in range(1, 6, 2):\n",
    "    vector_retriever = vector_index.as_retriever(similarity_top_k=k)\n",
    "    vector_evaluator = RetrieverEvaluator.from_metric_names(\n",
    "        metrics, retriever=vector_retriever\n",
    "    )\n",
    "    retriever_names.append(f\"Vector Index Top{k}\")\n",
    "    evaluators.append(vector_evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_id, sample_query = list(qa_dataset.queries.items())[0]\n",
    "sample_expected = qa_dataset.relevant_docs[sample_id]\n",
    "\n",
    "print(sample_id)\n",
    "print(sample_query)\n",
    "print(sample_expected)\n",
    "\n",
    "eval_result = evaluators[2].evaluate(sample_query, sample_expected)\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(names, eval_results):\n",
    "    \"\"\"Display results from evaluate.\"\"\"\n",
    "    \n",
    "    hit_rates = []\n",
    "    mrrs = []\n",
    "    if include_cohere_rerank:\n",
    "        crr_relevancys = []\n",
    "    \n",
    "    for eval_result in eval_results:\n",
    "        metric_dicts = []\n",
    "        for result in eval_result:\n",
    "            metric_dict = result.metric_vals_dict\n",
    "            metric_dicts.append(metric_dict)\n",
    "\n",
    "        full_df = pd.DataFrame(metric_dicts)\n",
    "\n",
    "        hit_rates.append(full_df[\"hit_rate\"].mean())\n",
    "        mrrs.append(full_df[\"mrr\"].mean())\n",
    "        if include_cohere_rerank:\n",
    "            crr_relevancys.append(full_df[\"cohere_rerank_relevancy\"].mean())\n",
    "\n",
    "        \n",
    "    columns = {\"retriever\": names, \"hit_rate\": hit_rates, \"mrr\": mrrs}\n",
    "    if include_cohere_rerank:\n",
    "        columns.update({\"cohere_rerank_relevancy\": crr_relevancys})\n",
    "    metric_df = pd.DataFrame(columns)\n",
    "\n",
    "    return metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = []\n",
    "\n",
    "start_time = time.time()\n",
    "for evaluator in evaluators:\n",
    "    eval_result = await evaluator.aevaluate_dataset(qa_dataset, show_progress=True)\n",
    "    eval_results.append(eval_result)\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Time taken for evaluation:\", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df = display_results(retriever_names, eval_results)\n",
    "metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df.loc[:, ['hit_rate', 'mrr']].plot.line()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
