{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Search + LLM\n",
    "\n",
    "> LLM as filterer of rankings provided by hybrid retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Create Interface to Interact with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "def initialize_pipeline(model_path: str, torch_dtype):\n",
    "    \"\"\"\n",
    "    Initialize a text generation pipeline\n",
    "\n",
    "    ### Parameters:\n",
    "    - model_path (str): The path of a model and tokenizer's weights.\n",
    "\n",
    "    ### Returns:\n",
    "    - pipe (TextGenerationPipeline): The pipeline for text generation.\n",
    "    \"\"\"\n",
    "\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model_path,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch_dtype\n",
    "    )\n",
    "\n",
    "    return pipe\n",
    "\n",
    "from transformers import set_seed\n",
    "\n",
    "def is_within_context_length(tokenizer, conversation, context_length: int):\n",
    "    # Check whether a conversation is within the context length\n",
    "    # after being tokenized.\n",
    "    conv_len = len(\n",
    "        tokenizer.apply_chat_template(\n",
    "            conversation, tokenize=True, add_generation_prompt=True\n",
    "        )\n",
    "    )\n",
    "    return conv_len <= (context_length)\n",
    "\n",
    "def validate_generation_configs(generation_configs):\n",
    "    if generation_configs[\"top_k\"] == 0:\n",
    "        del generation_configs[\"top_k\"]\n",
    "    if generation_configs[\"top_p\"] == 1.0:\n",
    "        del generation_configs[\"top_p\"]\n",
    "    if generation_configs[\"penalty_alpha\"] == 0.0:\n",
    "        del generation_configs[\"penalty_alpha\"]\n",
    "    if generation_configs[\"temperature\"] == 0.0:\n",
    "        del generation_configs[\"temperature\"]\n",
    "\n",
    "def prompt_pipeline(\n",
    "    pipe,\n",
    "    conversation,\n",
    "    context_length=8192,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=False,\n",
    "    top_k=0,\n",
    "    top_p=1.0,\n",
    "    penalty_alpha=0.0,\n",
    "    temperature=0.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Prompt the pipeline with a conversation\n",
    "\n",
    "    ### Parameters:\n",
    "    - pipe (TextGenerationPipeline): An initialized pipeline.\n",
    "    - conversation (list[dict[str, str]]): The data type of the model\n",
    "    - context_length (int): The LLM's context length\n",
    "    - max_new_tokens (int): Max number of tokens generated for each prompt\n",
    "    - do_sample (bool): Perform sampling or not\n",
    "    - top_k (int): The number of tokens to consider when sampling\n",
    "    - top_p (float): Minimum cumulative probability of tokens being considered\n",
    "    - penalty_alpha (float): The amount of focus being put to ensure non-repetitiveness\n",
    "    - temperature (float): Control how sharp the distribution (smaller means sharper)\n",
    "\n",
    "    ### Returns:\n",
    "    - conversation (list[dict[str, str]]): The conversation appended with the model's output\n",
    "    \"\"\"\n",
    "    generation_configs = {\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"top_k\": top_k,\n",
    "        \"top_p\": top_p,\n",
    "        \"do_sample\": do_sample,\n",
    "        \"penalty_alpha\": penalty_alpha,\n",
    "        \"temperature\": temperature,\n",
    "        \"pad_token_id\": pipe.tokenizer.eos_token_id\n",
    "    }\n",
    "    validate_generation_configs(generation_configs)\n",
    "    try:\n",
    "        if is_within_context_length(pipe.tokenizer, conversation, context_length):\n",
    "            set_seed(42)  # Enhance reproducibility\n",
    "            conversation = pipe(conversation, **generation_configs)[0][\"generated_text\"]\n",
    "            return conversation\n",
    "        else:\n",
    "            logger.warning(\n",
    "                \"The conversation is more than what the model can handle. Skip processing.\"\n",
    "            )\n",
    "            return [{\"role\": \"user\", \"content\": \"\"}]\n",
    "    except:\n",
    "        logger.warning(\n",
    "                \"The conversation is more than what the model can handle. Skip processing.\"\n",
    "            )\n",
    "        return [{\"role\": \"user\", \"content\": \"\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import (\n",
    "    Settings,\n",
    "    VectorStoreIndex,\n",
    "    Document,\n",
    ")\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# These packages are available in the benchmark_generator directory\n",
    "from pipeline.pipeline_initializer import initialize_pipeline\n",
    "from pipeline.prompting_interface import prompt_pipeline\n",
    "\n",
    "import torch\n",
    "import warnings\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize LLM and Index Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "pipe = initialize_pipeline(\"mistralai/Mistral-7B-Instruct-v0.3\", torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark = pd.read_csv(\"BX1_chicago_corrected.csv\")  # Adjust benchmark name\n",
    "benchmark.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_content_documents(df):\n",
    "    documents = []\n",
    "    for idx in df.index:\n",
    "        table = df[\"table\"][idx]\n",
    "        table_summary = df[\"summary\"][idx]\n",
    "        document = Document(\n",
    "            text=table_summary,\n",
    "            metadata={\"table\": table},\n",
    "            doc_id=f\"doc_'{table}'_{idx}\",\n",
    "        )\n",
    "        documents.append(document)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_context_documents(df):\n",
    "    documents = []\n",
    "    for idx in df.index:\n",
    "        table = df[\"table\"][idx]\n",
    "        answer = df[\"context\"][idx]\n",
    "        document = Document(\n",
    "            text=answer,\n",
    "            metadata={\"table\": table},\n",
    "            doc_id=f\"doc_'{table}'_{idx}\",\n",
    "        )\n",
    "        documents.append(document)\n",
    "    return documents\n",
    "documents = create_context_documents(benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index = VectorStoreIndex(documents)\n",
    "print(\"Index created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine BM25 & Vector Retrievers\n",
    "class HybridRetriever:\n",
    "    def __init__(self, bm25_retriever, vector_retriever, alpha=0.5):\n",
    "        self.bm25_retriever = bm25_retriever\n",
    "        self.vector_retriever = vector_retriever\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def process_nodes(self, nodes):\n",
    "        # Normalize relevance scores and return the nodes in dict format.\n",
    "        scores: list[float] = [node.score for node in nodes]\n",
    "        max_score = max(scores)\n",
    "        min_score = min(scores)\n",
    "        \n",
    "        processed_nodes = {}\n",
    "        for node in nodes:\n",
    "            if min_score == max_score:\n",
    "                node.score = 1\n",
    "            else:\n",
    "                node.score = (node.score - min_score) / (max_score - min_score)\n",
    "            processed_nodes[node.id_] = node\n",
    "        return processed_nodes\n",
    "\n",
    "    def retrieve(self, query, top_k=10):\n",
    "        # 1. Change the top_k of the retrievers\n",
    "        self.vector_retriever.similarity_top_k = top_k\n",
    "        self.bm25_retriever.similarity_top_k = top_k\n",
    "\n",
    "        # 2. Use both retrievers to get top-k results + normalization\n",
    "        bm25_nodes = self.process_nodes(self.bm25_retriever.retrieve(query))\n",
    "        vector_nodes = self.process_nodes(self.vector_retriever.retrieve(query))\n",
    "\n",
    "        # 3. Linearly combine the scores of each node\n",
    "        node_ids = set(list(bm25_nodes.keys()) + list(vector_nodes.keys()))\n",
    "        all_nodes = []\n",
    "        for node_id in node_ids:\n",
    "            try:\n",
    "                bm25_score = bm25_nodes.get(node_id).score\n",
    "            except:\n",
    "                bm25_score = 0.0\n",
    "            try:\n",
    "                cosine_score = vector_nodes.get(node_id).score\n",
    "            except:\n",
    "                cosine_score = 0.0\n",
    "            combined_score = self.alpha * bm25_score + (1 - self.alpha) * cosine_score\n",
    "            node = bm25_nodes.get(node_id, vector_nodes.get(node_id))\n",
    "            node.score = combined_score\n",
    "\n",
    "            all_nodes.append(node)\n",
    "        \n",
    "        sorted_nodes = sorted(all_nodes, key=lambda node: node.score, reverse=True)[:top_k]\n",
    "        return sorted_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevancy_prompt(metadata: str, query: str):\n",
    "    return f\"\"\"Metadata M:\"{metadata}\"\n",
    "Query Q: \"{query}\"\n",
    "Metadata M is associated with a dataset that we can access. Is this dataset relevant to query Q? Begin your argument with yes/no.\"\"\"\n",
    "\n",
    "def filter_retrieved_data(retrieved_data, query):\n",
    "    filtered_data = []\n",
    "    for data in retrieved_data:\n",
    "        metadata = data.text\n",
    "        prompt = get_relevancy_prompt(metadata, query)\n",
    "        conversation = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        model_output: str = prompt_pipeline(pipe, conversation, max_new_tokens=128, context_length=32768)[-1][\n",
    "            \"content\"\n",
    "        ]\n",
    "        # print(f\"DEBUG prompt: {prompt}\")\n",
    "        # print(\"=\" * 100)\n",
    "        # print(f\"DEBUG model output: {model_output}\")\n",
    "        # print(\"=\" * 100)\n",
    "        if model_output.lower().strip().startswith(\"yes\") or model_output.lower().strip().startswith(\"**yes**\"):\n",
    "            filtered_data.append(data)\n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_retrieved_data_to_tables_ranks(retrieved_data, query):\n",
    "    # Convert all retrieved data to the format (table, rank)\n",
    "    rank = 1\n",
    "    tables_ranks = []\n",
    "    filtered_data = filter_retrieved_data(retrieved_data, query)  # Filter with LLM\n",
    "    prev_score = -1000\n",
    "    for data in filtered_data:\n",
    "        if data.get_score() < prev_score:\n",
    "            rank += 1\n",
    "        table = data.id_.split(\"'\")[1]  # E.g., \"chicago_open_data/22u3-xenr\"\n",
    "        tables_ranks.append((table, rank))\n",
    "        prev_score = data.get_score()\n",
    "    return tables_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def evaluate(retriever, benchmark_df, top_k=1):\n",
    "    accuracy_sum = 0\n",
    "    precision_at_1_sum = 0\n",
    "    reciprocal_rank_sum = 0\n",
    "    for i in range(len(benchmark_df)):\n",
    "        query = benchmark_df[\"question\"][i]\n",
    "        expected_tables = ast.literal_eval(benchmark_df[\"relevant_tables\"][i])\n",
    "        retrieved_data = retriever.retrieve(query, top_k)  # Using hybrid retriever\n",
    "        tables_ranks = convert_retrieved_data_to_tables_ranks(retrieved_data, query)\n",
    "\n",
    "        before = accuracy_sum\n",
    "        for j, (table, rank) in enumerate(tables_ranks):\n",
    "            if table in expected_tables:\n",
    "                accuracy_sum += 1\n",
    "                if rank == 1:\n",
    "                    precision_at_1_sum += 1\n",
    "                reciprocal_rank_sum += 1 / (j + 1)\n",
    "                break\n",
    "        if accuracy_sum == before:\n",
    "            print(f\"Wrong answer at index {i}\")\n",
    "            # print(expected_tables)\n",
    "            # print(query)\n",
    "            # print(retrieved_data)\n",
    "\n",
    "        # Checkpointing\n",
    "        if i % 25 == 0:\n",
    "            print(f\"i: {i}\")\n",
    "            print(f\"accuracy_sum: {accuracy_sum}\")\n",
    "            print(f\"precision_at_1_sum: {precision_at_1_sum}\")\n",
    "            print(f\"reciprocal_rank_sum: {reciprocal_rank_sum}\")\n",
    "            print(\"=\" * 100)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_sum / benchmark_df.shape[0],\n",
    "        \"Mean Precision@1\": precision_at_1_sum / benchmark_df.shape[0],\n",
    "        \"MRR\": reciprocal_rank_sum / benchmark_df.shape[0],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "\n",
    "vector_retriever = vector_index.as_retriever()\n",
    "BM25_retriever = BM25Retriever.from_defaults(vector_index)\n",
    "hybrid_retriever = HybridRetriever(BM25_retriever, vector_retriever)\n",
    "\n",
    "result = evaluate(hybrid_retriever, benchmark, top_k=3)  # Adjust k\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
