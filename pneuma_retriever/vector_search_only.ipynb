{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import chromadb\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import set_seed\n",
    "from chromadb.api.client import Client\n",
    "from chromadb.api.models.Collection import Collection\n",
    "from benchmark_generator.context.utils.jsonl import read_jsonl\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.SentenceTransformer import SentenceTransformer\n",
    "\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "set_seed(42, deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer('../models/stella', local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"fetaqa\"\n",
    "if dataset == \"chicago\":\n",
    "    std_contents = read_jsonl(\"../pneuma_summarizer/summaries/standard/chicago_standard.jsonl\")\n",
    "    narration_contents = read_jsonl(\"../pneuma_summarizer/summaries/narrations/chicago_narrations.jsonl\")\n",
    "    contexts = read_jsonl(\"../data_src/benchmarks/context/chicago/contexts_chicago.jsonl\")\n",
    "\n",
    "    content_benchmark = read_jsonl(\"../data_src/benchmarks/content/pneuma_chicago_10K_questions_annotated.jsonl\")\n",
    "    context_benchmark = read_jsonl(\"../data_src/benchmarks/context/chicago/bx_chicago.jsonl\")\n",
    "    path = \"../data_src/tables/pneuma_chicago_10K\"\n",
    "elif dataset == \"public\":\n",
    "    std_contents = read_jsonl(\"../pneuma_summarizer/summaries/standard/public_standard.jsonl\")\n",
    "    narration_contents = read_jsonl(\"../pneuma_summarizer/summaries/narrations/public_narrations.jsonl\")\n",
    "    contexts = read_jsonl(\"../data_src/benchmarks/context/public/contexts_public.jsonl\")\n",
    "\n",
    "    content_benchmark = read_jsonl(\"../data_src/benchmarks/content/pneuma_public_bi_questions_annotated.jsonl\")\n",
    "    context_benchmark = read_jsonl(\"../data_src/benchmarks/context/public/bx_public.jsonl\")\n",
    "    path = \"../data_src/tables/pneuma_public_bi\"\n",
    "elif dataset == \"chembl\":\n",
    "    std_contents = read_jsonl(\"../pneuma_summarizer/summaries/standard/chembl_standard.jsonl\")\n",
    "    narration_contents = read_jsonl(\"../pneuma_summarizer/summaries/narrations/chembl_narrations.jsonl\")\n",
    "    contexts = read_jsonl(\"../data_src/benchmarks/context/chembl/contexts_chembl.jsonl\")\n",
    "\n",
    "    content_benchmark = read_jsonl(\"../data_src/benchmarks/content/pneuma_chembl_10K_questions_annotated.jsonl\")\n",
    "    context_benchmark = read_jsonl(\"../data_src/benchmarks/context/chembl/bx_chembl.jsonl\")\n",
    "    path = \"../data_src/tables/pneuma_chembl_10K\"\n",
    "elif dataset == \"adventure\":\n",
    "    std_contents = read_jsonl(\"../pneuma_summarizer/summaries/standard/adventure_standard.jsonl\")\n",
    "    narration_contents = read_jsonl(\"../pneuma_summarizer/summaries/narrations/adventure_narrations.jsonl\")\n",
    "    contexts = read_jsonl(\"../data_src/benchmarks/context/adventure/contexts_adventure.jsonl\")\n",
    "\n",
    "    content_benchmark = read_jsonl(\"../data_src/benchmarks/content/pneuma_adventure_works_questions_annotated.jsonl\")\n",
    "    context_benchmark = read_jsonl(\"../data_src/benchmarks/context/adventure/bx_adventure.jsonl\")\n",
    "    path = \"../data_src/tables/pneuma_adventure_works\"\n",
    "elif dataset == \"fetaqa\":\n",
    "    std_contents = read_jsonl(\"../pneuma_summarizer/summaries/standard/fetaqa_standard.jsonl\")\n",
    "    narration_contents = read_jsonl(\"../pneuma_summarizer/summaries/narrations/fetaqa_narrations.jsonl\")\n",
    "    contexts = read_jsonl(\"../data_src/benchmarks/context/fetaqa/contexts_fetaqa.jsonl\")\n",
    "    content_benchmark = read_jsonl(\"../data_src/benchmarks/content/pneuma_fetaqa_questions_annotated.jsonl\")\n",
    "    context_benchmark = read_jsonl(\"../data_src/benchmarks/context/fetaqa/bx_fetaqa.jsonl\")\n",
    "    path = \"../data_src/tables/pneuma_fetaqa\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexing_vector(\n",
    "    client: Client,\n",
    "    embedding_model: SentenceTransformer,\n",
    "    std_contents: list[dict[str, str]],\n",
    "    contexts: list[dict[str, str]] = None,\n",
    "    collection_name = \"benchmark\",\n",
    "    reindex = False,\n",
    "):\n",
    "    documents = []\n",
    "    metadatas = []\n",
    "    ids = []\n",
    "\n",
    "    if not reindex:\n",
    "        try:\n",
    "            collection = client.get_collection(collection_name)\n",
    "            return collection\n",
    "        except:\n",
    "            pass\n",
    "    try:\n",
    "        client.delete_collection(collection_name)\n",
    "    except:\n",
    "        pass\n",
    "    collection = client.create_collection(\n",
    "        name=collection_name,\n",
    "        metadata={\"hnsw:space\": \"cosine\", \"hnsw:random_seed\": 42}\n",
    "    )\n",
    "\n",
    "    tables = sorted({content[\"table\"] for content in std_contents})\n",
    "    for table in tables:\n",
    "        cols = [content[\"summary\"] for content in std_contents if content[\"table\"] == table]\n",
    "        for content_idx, content in enumerate(cols):\n",
    "            documents.append(content)\n",
    "            metadatas.append({\"table\": f\"{table}_SEP_contents_{content_idx}\"})\n",
    "            ids.append(f\"{table}_SEP_contents_{content_idx}\")\n",
    "\n",
    "        if contexts is not None:\n",
    "            filtered_contexts = [context[\"context\"] for context in contexts if context[\"table\"] == table]\n",
    "            for context_idx, context in enumerate(filtered_contexts):\n",
    "                documents.append(context)\n",
    "                metadatas.append({\"table\": f\"{table}_SEP_{context_idx}\"})\n",
    "                ids.append(f\"{table}_SEP_{context_idx}\")\n",
    "\n",
    "    for i in range(0, len(documents), 30000):\n",
    "        try:\n",
    "            embeddings = np.loadtxt(\n",
    "                f\"embeddings/embed-{dataset}-context-content-{i}.txt\"\n",
    "            )\n",
    "        except:\n",
    "            embeddings = embedding_model.encode(\n",
    "                documents[i:i+30000],\n",
    "                batch_size=64,\n",
    "                show_progress_bar=True,\n",
    "                device=\"cuda\"\n",
    "            )\n",
    "            np.savetxt(\n",
    "                f\"embeddings/embed-{dataset}-context-content-{i}.txt\", embeddings\n",
    "            )\n",
    "\n",
    "        collection.add(\n",
    "            embeddings=[embed.tolist() for embed in embeddings],\n",
    "            metadatas=metadatas[i:i+30000],\n",
    "            documents=documents[i:i+30000],\n",
    "            ids=ids[i:i+30000],\n",
    "        )\n",
    "    return collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "client = chromadb.PersistentClient(f\"indices/index-{dataset}-context-content\")\n",
    "collection = indexing_vector(client, embedding_model, std_contents, contexts)\n",
    "end = time.time()\n",
    "print(f\"Indexing time: {end-start} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_benchmark(\n",
    "    benchmark: list[dict[str,str]],\n",
    "    benchmark_type: str,\n",
    "    k: int,\n",
    "    embedding_model: SentenceTransformer,\n",
    "    collection: Collection,\n",
    "    use_rephrased_questions=False\n",
    "):\n",
    "    start = time.time()\n",
    "    hitrate_sum = 0\n",
    "    wrong_questions = []\n",
    "    def get_question_key(benchmark_type: str):\n",
    "        if benchmark_type == \"content\":\n",
    "            if not use_rephrased_questions:\n",
    "                question_key = \"question_from_sql_1\"\n",
    "            else:\n",
    "                question_key = \"question\"\n",
    "        else:\n",
    "            if not use_rephrased_questions:\n",
    "                question_key = \"question_bx1\"\n",
    "            else:\n",
    "                question_key = \"question_bx2\"\n",
    "        return question_key\n",
    "    question_key = get_question_key(benchmark_type)\n",
    "\n",
    "    questions = []\n",
    "    for data in benchmark:\n",
    "        questions.append(data[question_key])\n",
    "    try:\n",
    "        embed_questions = np.loadtxt(\n",
    "            f\"embeddings/embed-{dataset}-questions-{benchmark_type}-{use_rephrased_questions}.txt\"\n",
    "        )\n",
    "    except:\n",
    "        embed_questions = embedding_model.encode(questions, batch_size=64, show_progress_bar=True)\n",
    "        np.savetxt(\n",
    "            f\"embeddings/embed-{dataset}-questions-{benchmark_type}-{use_rephrased_questions}.txt\",\n",
    "            embed_questions,\n",
    "        )\n",
    "    embed_questions = [embed.tolist() for embed in embed_questions]\n",
    "\n",
    "    for idx, datum in enumerate(tqdm(benchmark)):\n",
    "        answer_tables = datum[\"answer_tables\"]\n",
    "        vec_res = collection.query(\n",
    "            query_embeddings=[embed_questions[idx]],\n",
    "            n_results=k\n",
    "        )\n",
    "        before = hitrate_sum\n",
    "        for res in vec_res['ids'][0]:\n",
    "            table = res.split(\"_SEP_\")[0]\n",
    "            if table in answer_tables:\n",
    "                hitrate_sum += 1\n",
    "                break\n",
    "        if before == hitrate_sum:\n",
    "            wrong_questions.append(idx)\n",
    "    \n",
    "    end = time.time()\n",
    "\n",
    "    print(f\"Hit Rate: {hitrate_sum/len(benchmark) * 100}\")\n",
    "    print(f\"Benchmarking Time: {end - start} seconds\")\n",
    "    print(f\"Wrongly answered questions: {wrong_questions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BC1\n",
    "evaluate_benchmark(\n",
    "    content_benchmark, \"content\", 1, embedding_model, collection\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BC2\n",
    "evaluate_benchmark(\n",
    "    content_benchmark, \"content\", 1, embedding_model, collection, use_rephrased_questions=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BX1\n",
    "evaluate_benchmark(\n",
    "    context_benchmark, \"context\", 1, embedding_model, collection\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BX2\n",
    "evaluate_benchmark(\n",
    "    context_benchmark, \"context\", 1, embedding_model, collection, use_rephrased_questions=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summary-neo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
