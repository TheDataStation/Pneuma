{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import bm25s\n",
    "import Stemmer\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "from benchmark_generator.context.utils.jsonl import read_jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"chembl\"\n",
    "if dataset == \"chicago\":\n",
    "    std_contents = read_jsonl(\"../../pneuma_summarizer/summaries/standard/chicago_standard.jsonl\")\n",
    "    narration_contents = read_jsonl(\"../../pneuma_summarizer/summaries/narrations/chicago_narrations.jsonl\")\n",
    "    content_benchmark = read_jsonl(\"../../data_src/benchmarks/content/pneuma_chicago_10K_questions_annotated.jsonl\")\n",
    "    path = \"../../data_src/tables/pneuma_chicago_10K\"\n",
    "elif dataset == \"public\":\n",
    "    std_contents = read_jsonl(\"../../pneuma_summarizer/summaries/standard/public_standard.jsonl\")\n",
    "    narration_contents = read_jsonl(\"../../pneuma_summarizer/summaries/narrations/public_narrations.jsonl\")\n",
    "    content_benchmark = read_jsonl(\"../../data_src/benchmarks/content/pneuma_public_bi_questions_annotated.jsonl\")\n",
    "    path = \"../../data_src/tables/pneuma_public_bi\"\n",
    "elif dataset == \"chembl\":\n",
    "    std_contents = read_jsonl(\"../../pneuma_summarizer/summaries/standard/chembl_standard.jsonl\")\n",
    "    narration_contents = read_jsonl(\"../../pneuma_summarizer/summaries/narrations/chembl_narrations.jsonl\")\n",
    "    content_benchmark = read_jsonl(\"../../data_src/benchmarks/content/pneuma_chembl_10K_questions_annotated.jsonl\")\n",
    "    path = \"../../data_src/tables/pneuma_chembl_10K\"\n",
    "elif dataset == \"adventure\":\n",
    "    std_contents = read_jsonl(\"../../pneuma_summarizer/summaries/standard/adventure_standard.jsonl\")\n",
    "    narration_contents = read_jsonl(\"../../pneuma_summarizer/summaries/narrations/adventure_narrations.jsonl\")\n",
    "    content_benchmark = read_jsonl(\"../../data_src/benchmarks/content/pneuma_adventure_works_questions_annotated.jsonl\")\n",
    "    path = \"../../data_src/tables/pneuma_adventure_works\"\n",
    "elif dataset == \"fetaqa\":\n",
    "    std_contents = read_jsonl(\"../../pneuma_summarizer/summaries/standard/adventure_standard.jsonl\")\n",
    "    narration_contents = read_jsonl(\"../../pneuma_summarizer/summaries/narrations/adventure_narrations.jsonl\")\n",
    "    content_benchmark = read_jsonl(\"../../data_src/benchmarks/content/pneuma_fetaqa_questions_annotated.jsonl\")\n",
    "    path = \"../../data_src/tables/pneuma_fetaqa\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexing_keyword(\n",
    "    stemmer,\n",
    "    narration_contents: list[dict[str, str]],\n",
    "    contexts: list[dict[str, str]] = None,\n",
    "):\n",
    "    corpus_json = []\n",
    "    tables = sorted({content[\"table\"] for content in narration_contents})\n",
    "    for table in tables:\n",
    "        cols_descriptions = [content[\"summary\"] for content in narration_contents if content[\"table\"] == table]\n",
    "        for content_idx, content in enumerate(cols_descriptions):\n",
    "            corpus_json.append({\"text\": content, \"metadata\": {\"table\": f\"{table}_SEP_contents_{content_idx}\"}})\n",
    "\n",
    "        if contexts is not None:\n",
    "            filtered_contexts = [context[\"context\"] for context in contexts if context[\"table\"] == table]\n",
    "            for context_idx, context in enumerate(filtered_contexts):\n",
    "                corpus_json.append({\"text\": context, \"metadata\": {\"table\": f\"{table}_SEP_{context_idx}\"}})\n",
    "\n",
    "    corpus_text = [doc[\"text\"] for doc in corpus_json]\n",
    "    corpus_tokens = bm25s.tokenize(corpus_text, stopwords=\"en\", stemmer=stemmer, show_progress=False)\n",
    "\n",
    "    retriever = bm25s.BM25(corpus=corpus_json)\n",
    "    retriever.index(corpus_tokens, show_progress=False)\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = Stemmer.Stemmer(\"english\")\n",
    "retriever = indexing_keyword(stemmer, std_contents, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_benchmark(\n",
    "    benchmark: list[dict[str,str]],\n",
    "    benchmark_type: str,\n",
    "    k: int,\n",
    "    retriever,\n",
    "    stemmer,\n",
    "    use_rephrased_questions=False\n",
    "):\n",
    "    hitrate_sum = 0\n",
    "    \n",
    "    def get_question_key(benchmark_type: str):\n",
    "        if benchmark_type == \"content\":\n",
    "            if not use_rephrased_questions:\n",
    "                question_key = \"question_from_sql_1\"\n",
    "            else:\n",
    "                question_key = \"question\"\n",
    "        else:\n",
    "            if not use_rephrased_questions:\n",
    "                question_key = \"question_bx1\"\n",
    "            else:\n",
    "                question_key = \"question_bx2\"\n",
    "        return question_key\n",
    "    question_key = get_question_key(benchmark_type)\n",
    "\n",
    "    questions = []\n",
    "    for data in benchmark:\n",
    "        questions.append(data[question_key])\n",
    "\n",
    "    for idx, datum in enumerate(tqdm(benchmark)):\n",
    "        answer_tables = datum[\"answer_tables\"]\n",
    "\n",
    "        query_tokens = bm25s.tokenize(questions[idx], stemmer=stemmer, show_progress=False)\n",
    "        results, scores = retriever.retrieve(query_tokens, k=k, show_progress=False)\n",
    "        for result in results[0]:\n",
    "            table = result['metadata']['table'].split(\"_SEP_\")[0]\n",
    "            if table in answer_tables:\n",
    "                hitrate_sum += 1\n",
    "                break\n",
    "    print(f\"Hit Rate: {round(hitrate_sum/len(benchmark) * 100, 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = [1,5,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BC1\n",
    "for k in ks:\n",
    "    print(f\"k={k}\")\n",
    "    evaluate_benchmark(\n",
    "        content_benchmark, \"content\", k, retriever, stemmer\n",
    "    )\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BC2\n",
    "for k in ks:\n",
    "    print(f\"k={k}\")\n",
    "    evaluate_benchmark(\n",
    "        content_benchmark, \"content\", k, retriever, stemmer, True\n",
    "    )\n",
    "    print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summary-neo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
