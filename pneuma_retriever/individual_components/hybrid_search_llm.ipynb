{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/p-storage/luthfibalaka_d2b65c57/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import setproctitle\n",
    "\n",
    "setproctitle.setproctitle(\"python\")\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import chromadb\n",
    "import bm25s\n",
    "import Stemmer\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from transformers import set_seed\n",
    "from chromadb.api.models.Collection import Collection\n",
    "from benchmark_generator.context.utils.jsonl import read_jsonl\n",
    "from benchmark_generator.context.utils.pipeline_initializer import initialize_pipeline\n",
    "from benchmark_generator.context.utils.prompting_interface import prompt_pipeline\n",
    "\n",
    "\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "set_seed(42, deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe = initialize_pipeline(\"../../models/llama\", torch.bfloat16)\n",
    "stemmer = Stemmer.Stemmer(\"english\")\n",
    "\n",
    "# # Specific setting for Llama-3-8B-Instruct for batching\n",
    "# pipe.tokenizer.pad_token_id = pipe.model.config.eos_token_id\n",
    "# pipe.tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"adventure\"\n",
    "if dataset == \"chicago\":\n",
    "    narrations = read_jsonl(\"../../pneuma_summarizer/summaries/narrations/chicago_narrations.jsonl\")\n",
    "    rows = read_jsonl(\"../../pneuma_summarizer/summaries/rows/chicago.jsonl\")\n",
    "    content_benchmark = read_jsonl(\"../../data_src/benchmarks/content/pneuma_chicago_10K_questions_annotated.jsonl\")\n",
    "    path = \"../../data_src/tables/pneuma_chicago_10K\"\n",
    "elif dataset == \"public\":\n",
    "    narrations = read_jsonl(\"../../pneuma_summarizer/summaries/narrations/public_narrations.jsonl\")\n",
    "    rows = read_jsonl(\"../../pneuma_summarizer/summaries/rows/public.jsonl\")\n",
    "    content_benchmark = read_jsonl(\"../../data_src/benchmarks/content/pneuma_public_bi_questions_annotated.jsonl\")\n",
    "    path = \"../../data_src/tables/pneuma_public_bi\"\n",
    "elif dataset == \"chembl\":\n",
    "    narrations = read_jsonl(\"../../pneuma_summarizer/summaries/narrations/chembl_narrations.jsonl\")\n",
    "    rows = read_jsonl(\"../../pneuma_summarizer/summaries/rows/chembl.jsonl\")\n",
    "    contexts = read_jsonl(\"../../data_src/benchmarks/context/chembl/contexts_chembl.jsonl\")\n",
    "    content_benchmark = read_jsonl(\"../../data_src/benchmarks/content/pneuma_chembl_10K_questions_annotated.jsonl\")\n",
    "    context_benchmark = read_jsonl(\"../../data_src/benchmarks/context/chembl/bx_chembl.jsonl\")\n",
    "    path = \"../../data_src/tables/pneuma_chembl_10K\"\n",
    "elif dataset == \"adventure\":\n",
    "    narrations = read_jsonl(\"../../pneuma_summarizer/summaries/narrations/adventure_narrations.jsonl\")\n",
    "    rows = read_jsonl(\"../../pneuma_summarizer/summaries/rows/adventure.jsonl\")\n",
    "    contexts = read_jsonl(\"../../data_src/benchmarks/context/adventure/contexts_adventure.jsonl\")\n",
    "    content_benchmark = read_jsonl(\"../../data_src/benchmarks/content/pneuma_adventure_works_questions_annotated.jsonl\")\n",
    "    context_benchmark = read_jsonl(\"../../data_src/benchmarks/context/adventure/bx_adventure.jsonl\")\n",
    "    path = \"../../data_src/tables/pneuma_adventure_works\"\n",
    "elif dataset == \"fetaqa\":\n",
    "    narrations = read_jsonl(\"../../pneuma_summarizer/summaries/narrations/fetaqa_narrations.jsonl\")\n",
    "    rows = read_jsonl(\"../../pneuma_summarizer/summaries/rows/fetaqa.jsonl\")\n",
    "    content_benchmark = read_jsonl(\"../../data_src/benchmarks/content/pneuma_fetaqa_questions_annotated.jsonl\")\n",
    "    path = \"../../data_src/tables/pneuma_fetaqa\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexing_keyword(\n",
    "    stemmer,\n",
    "    narration_contents: list[dict[str, str]],\n",
    "    contexts: list[dict[str, str]] = None,\n",
    "):\n",
    "    corpus_json = []\n",
    "    tables = sorted({content[\"table\"] for content in narration_contents})\n",
    "    for table in tables:\n",
    "        cols_descriptions = [\n",
    "            content[\"summary\"]\n",
    "            for content in narration_contents\n",
    "            if content[\"table\"] == table\n",
    "        ]\n",
    "        for content_idx, content in enumerate(cols_descriptions):\n",
    "            corpus_json.append(\n",
    "                {\n",
    "                    \"text\": content,\n",
    "                    \"metadata\": {\"table\": f\"{table}_SEP_contents_{content_idx}\"},\n",
    "                }\n",
    "            )\n",
    "\n",
    "        if contexts is not None:\n",
    "            filtered_contexts = [\n",
    "                context[\"context\"] for context in contexts if context[\"table\"] == table\n",
    "            ]\n",
    "            for context_idx, context in enumerate(filtered_contexts):\n",
    "                corpus_json.append(\n",
    "                    {\n",
    "                        \"text\": context,\n",
    "                        \"metadata\": {\"table\": f\"{table}_SEP_{context_idx}\"},\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    corpus_text = [doc[\"text\"] for doc in corpus_json]\n",
    "    corpus_tokens = bm25s.tokenize(\n",
    "        corpus_text, stopwords=\"en\", stemmer=stemmer, show_progress=False\n",
    "    )\n",
    "\n",
    "    retriever = bm25s.BM25(corpus=corpus_json)\n",
    "    retriever.index(corpus_tokens, show_progress=False)\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing adventure dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:bm25s:Building index from IDs objects\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing time: 0.563671350479126 seconds\n"
     ]
    }
   ],
   "source": [
    "print(f\"Processing {dataset} dataset\")\n",
    "start = time.time()\n",
    "client = chromadb.PersistentClient(\n",
    "    f\"../indices/index-{dataset}-pneuma-summarizer\"\n",
    ")\n",
    "collection = client.get_collection(\"benchmark\")\n",
    "retriever = indexing_keyword(stemmer, rows + narrations, contexts)\n",
    "end = time.time()\n",
    "print(f\"Indexing time: {end-start} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_nodes_bm25(items):\n",
    "    # Normalize relevance scores and return the nodes in dict format\n",
    "    results, scores = items\n",
    "    scores: list[float] = scores[0]\n",
    "    max_score = max(scores)\n",
    "    min_score = min(scores)\n",
    "\n",
    "    processed_nodes: dict[str, tuple[float, str]] = {}\n",
    "    for i, node in enumerate(results[0]):\n",
    "        if min_score == max_score:\n",
    "            score = 1\n",
    "        else:\n",
    "            score = (scores[i] - min_score) / (max_score - min_score)\n",
    "        processed_nodes[node[\"metadata\"][\"table\"]] = (score, node[\"text\"])\n",
    "    return processed_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_nodes_vec(items):\n",
    "    # Normalize relevance scores and return the nodes in dict format\n",
    "    scores: list[float] = [1 - dist for dist in items[\"distances\"][0]]\n",
    "    max_score = max(scores)\n",
    "    min_score = min(scores)\n",
    "\n",
    "    processed_nodes: dict[str, tuple[float, str]] = {}\n",
    "\n",
    "    for idx in range(len(items[\"ids\"][0])):\n",
    "        if min_score == max_score:\n",
    "            score = 1\n",
    "        else:\n",
    "            score = (scores[idx] - min_score) / (max_score - min_score)\n",
    "        processed_nodes[items[\"ids\"][0][idx]] = (score, items[\"documents\"][0][idx])\n",
    "    return processed_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_retriever(\n",
    "    bm25_res,\n",
    "    vec_res,\n",
    "    k: int,\n",
    "    question: str,\n",
    "    use_reranker=False,\n",
    "):\n",
    "    processed_nodes_bm25 = process_nodes_bm25(bm25_res)\n",
    "    processed_nodes_vec = process_nodes_vec(vec_res)\n",
    "\n",
    "    node_ids = set(list(processed_nodes_bm25.keys()) + list(processed_nodes_vec.keys()))\n",
    "    all_nodes: list[tuple[str, float, str]] = []\n",
    "    for node_id in sorted(node_ids):\n",
    "        bm25_score_doc = processed_nodes_bm25.get(node_id, (0.0, None))\n",
    "        vec_score_doc = processed_nodes_vec.get(node_id, (0.0, None))\n",
    "\n",
    "        combined_score = 0.5 * bm25_score_doc[0] + 0.5 * vec_score_doc[0]\n",
    "        if bm25_score_doc[1] is None:\n",
    "            doc = vec_score_doc[1]\n",
    "        else:\n",
    "            doc = bm25_score_doc[1]\n",
    "\n",
    "        all_nodes.append((node_id, combined_score, doc))\n",
    "\n",
    "    sorted_nodes = sorted(all_nodes, key=lambda node: (-node[1], node[0]))[:k]\n",
    "    if use_reranker:\n",
    "        sorted_nodes = rerank(sorted_nodes, question)\n",
    "    return sorted_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevance_prompt(desc: str, desc_type: str, question: str):\n",
    "    if desc_type == \"content\":\n",
    "        return f\"\"\"Given a table with the following columns:\n",
    "*/\n",
    "{desc}\n",
    "*/\n",
    "and this question:\n",
    "/*\n",
    "{question}\n",
    "*/\n",
    "Is the table relevant to answer the question? Begin your answer with yes/no.\"\"\"\n",
    "    elif desc_type == \"context\":\n",
    "        return f\"\"\"Given this context describing a table:\n",
    "*/\n",
    "{desc}\n",
    "*/\n",
    "and this question:\n",
    "/*\n",
    "{question}\n",
    "*/\n",
    "Is the table relevant to answer the question? Begin your answer with yes/no.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank(nodes: list[tuple[str, float, str]], question: str):\n",
    "    tables_relevance = defaultdict(bool)\n",
    "    relevance_prompts = []\n",
    "    node_tables = []\n",
    "\n",
    "    for node in nodes:\n",
    "        table_name = node[0]\n",
    "        node_tables.append(table_name)\n",
    "        if table_name.split(\"_SEP_\")[1].startswith(\"contents\"):\n",
    "            relevance_prompts.append(\n",
    "                [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": get_relevance_prompt(node[2], \"content\", question),\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            relevance_prompts.append(\n",
    "                [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": get_relevance_prompt(node[2], \"context\", question),\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    arguments = prompt_pipeline(\n",
    "        pipe,\n",
    "        relevance_prompts,\n",
    "        batch_size=1,\n",
    "        context_length=8192,\n",
    "        max_new_tokens=2,\n",
    "        top_p=None,\n",
    "        temperature=None,\n",
    "    )\n",
    "    for arg_idx, argument in enumerate(arguments):\n",
    "        if argument[-1][\"content\"].lower().startswith(\"yes\"):\n",
    "            tables_relevance[node_tables[arg_idx]] = True\n",
    "\n",
    "    new_nodes = [\n",
    "        (table_name, score, doc)\n",
    "        for table_name, score, doc in nodes\n",
    "        if tables_relevance[table_name]\n",
    "    ] + [\n",
    "        (table_name, score, doc)\n",
    "        for table_name, score, doc in nodes\n",
    "        if not tables_relevance[table_name]\n",
    "    ]\n",
    "    return new_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_question_key(benchmark_type: str, use_rephrased_questions: bool):\n",
    "    if benchmark_type == \"content\":\n",
    "        if not use_rephrased_questions:\n",
    "            question_key = \"question_from_sql_1\"\n",
    "        else:\n",
    "            question_key = \"question\"\n",
    "    else:\n",
    "        if not use_rephrased_questions:\n",
    "            question_key = \"question_bx1\"\n",
    "        else:\n",
    "            question_key = \"question_bx2\"\n",
    "    return question_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_benchmark(\n",
    "    benchmark: list[dict[str, str]],\n",
    "    benchmark_type: str,\n",
    "    k: int,\n",
    "    collection: Collection,\n",
    "    retriever,\n",
    "    stemmer,\n",
    "    n=3,\n",
    "    use_reranker=False,\n",
    "    use_rephrased_questions=False,\n",
    "):\n",
    "    start = time.time()\n",
    "    hitrate_sum = 0\n",
    "    wrong_questions = []\n",
    "\n",
    "    if use_reranker:\n",
    "        increased_k = k * n\n",
    "    else:\n",
    "        increased_k = k * n\n",
    "\n",
    "    question_key = get_question_key(benchmark_type, use_rephrased_questions)\n",
    "\n",
    "    questions = []\n",
    "    for data in benchmark:\n",
    "        questions.append(data[question_key])\n",
    "    embed_questions = np.loadtxt(\n",
    "        f\"../embeddings/embed-{dataset}-questions-{benchmark_type}-{use_rephrased_questions}.txt\"\n",
    "    )\n",
    "    embed_questions = [embed.tolist() for embed in embed_questions]\n",
    "\n",
    "    for idx, datum in enumerate(tqdm(benchmark)):\n",
    "        answer_tables = datum[\"answer_tables\"]\n",
    "        question_embedding = embed_questions[idx]\n",
    "\n",
    "        query_tokens = bm25s.tokenize(\n",
    "            questions[idx], stemmer=stemmer, show_progress=False\n",
    "        )\n",
    "        results, scores = retriever.retrieve(\n",
    "            query_tokens, k=increased_k, show_progress=False\n",
    "        )\n",
    "        bm25_res = (results, scores)\n",
    "\n",
    "        vec_res = collection.query(\n",
    "            query_embeddings=[question_embedding], n_results=increased_k\n",
    "        )\n",
    "\n",
    "        all_nodes = hybrid_retriever(\n",
    "            bm25_res, vec_res, increased_k, questions[idx], use_reranker\n",
    "        )\n",
    "        before = hitrate_sum\n",
    "        for table, _, _ in all_nodes[:k]:\n",
    "            table = table.split(\"_SEP_\")[0]\n",
    "            if table in answer_tables:\n",
    "                hitrate_sum += 1\n",
    "                break\n",
    "        if before == hitrate_sum:\n",
    "            wrong_questions.append(idx)\n",
    "        # Checkpoint\n",
    "        # if idx % 20 == 0:\n",
    "        #     print(f\"Current Hit Rate Sum at index {idx}: {hitrate_sum}\")\n",
    "        #     print(\n",
    "        #         f\"Current wrongly answered questions at index {idx}: {wrong_questions}\"\n",
    "        #     )\n",
    "\n",
    "    end = time.time()\n",
    "    print(f\"Hit Rate: {round(hitrate_sum/len(benchmark) * 100, 2)}\")\n",
    "    print(f\"Benchmarking Time: {end - start} seconds\")\n",
    "    print(f\"Wrongly answered questions: {wrong_questions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = [1]\n",
    "ns = [5]\n",
    "use_reranker = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BC1 with k=1 n=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 309.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit Rate: 56.7\n",
      "Benchmarking Time: 3.613685131072998 seconds\n",
      "Wrongly answered questions: [1, 2, 5, 6, 8, 9, 11, 14, 16, 17, 26, 27, 28, 29, 30, 31, 34, 35, 37, 38, 39, 41, 42, 43, 44, 48, 54, 58, 64, 65, 68, 70, 71, 77, 92, 96, 97, 98, 99, 100, 101, 103, 105, 106, 110, 111, 112, 114, 117, 121, 122, 125, 136, 137, 138, 139, 143, 144, 146, 147, 151, 152, 153, 156, 159, 162, 163, 165, 166, 171, 172, 177, 179, 183, 184, 186, 187, 193, 196, 197, 198, 203, 208, 210, 212, 213, 214, 220, 228, 229, 231, 234, 237, 238, 239, 243, 244, 245, 246, 251, 252, 255, 256, 257, 258, 263, 266, 267, 268, 269, 272, 273, 274, 275, 276, 277, 280, 281, 282, 283, 284, 289, 292, 305, 306, 307, 309, 311, 312, 313, 319, 322, 323, 325, 326, 327, 328, 334, 336, 337, 339, 342, 344, 347, 352, 354, 356, 363, 365, 370, 371, 373, 374, 375, 376, 377, 382, 384, 387, 388, 390, 391, 394, 395, 397, 398, 399, 401, 407, 409, 411, 412, 413, 419, 420, 421, 424, 426, 428, 441, 446, 449, 456, 458, 460, 462, 463, 465, 469, 477, 482, 485, 486, 488, 489, 490, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 506, 508, 509, 510, 512, 513, 514, 515, 517, 518, 519, 521, 522, 523, 525, 533, 535, 536, 537, 538, 539, 540, 544, 549, 551, 552, 554, 555, 558, 559, 568, 570, 576, 577, 578, 579, 581, 583, 585, 586, 588, 589, 591, 593, 594, 596, 598, 603, 605, 609, 612, 613, 614, 615, 616, 618, 619, 620, 621, 623, 624, 625, 629, 630, 631, 635, 636, 638, 640, 641, 642, 645, 651, 653, 654, 655, 656, 657, 658, 659, 660, 661, 663, 669, 670, 671, 677, 679, 684, 685, 687, 688, 689, 693, 694, 695, 696, 697, 698, 700, 701, 702, 707, 710, 711, 715, 717, 721, 723, 731, 732, 733, 734, 735, 736, 742, 743, 744, 745, 746, 748, 749, 753, 756, 757, 762, 763, 766, 767, 768, 769, 770, 772, 773, 776, 779, 783, 789, 790, 791, 793, 794, 795, 798, 800, 807, 808, 809, 811, 813, 814, 816, 817, 818, 819, 820, 821, 822, 826, 830, 836, 840, 848, 850, 854, 855, 870, 871, 872, 873, 874, 875, 876, 877, 878, 886, 887, 892, 894, 896, 897, 898, 909, 912, 913, 915, 917, 925, 926, 928, 931, 933, 934, 935, 941, 943, 944, 945, 947, 949, 950, 951, 952, 955, 960, 961, 962, 963, 967, 968, 970, 972, 973, 975, 976, 977, 978, 979, 983, 984, 985, 987, 989, 991, 993, 994, 997]\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for k in ks:\n",
    "    for n in ns:\n",
    "        print(f\"BC1 with k={k} n={n}\")\n",
    "        evaluate_benchmark(\n",
    "            content_benchmark,\n",
    "            \"content\",\n",
    "            k,\n",
    "            collection,\n",
    "            retriever,\n",
    "            stemmer,\n",
    "            n=n,\n",
    "            use_reranker=use_reranker,\n",
    "        )\n",
    "        print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BC2 with k=1 n=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:02<00:00, 340.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit Rate: 54.2\n",
      "Benchmarking Time: 3.3353850841522217 seconds\n",
      "Wrongly answered questions: [0, 1, 2, 5, 6, 7, 8, 9, 11, 13, 14, 16, 17, 19, 21, 24, 26, 27, 28, 29, 32, 33, 36, 37, 39, 40, 41, 42, 43, 44, 48, 51, 52, 54, 57, 58, 65, 66, 68, 70, 71, 77, 78, 96, 97, 98, 99, 100, 101, 104, 105, 106, 107, 108, 110, 111, 113, 114, 118, 122, 125, 126, 128, 130, 131, 137, 138, 139, 144, 145, 146, 147, 148, 150, 151, 152, 153, 154, 156, 157, 158, 159, 160, 161, 163, 164, 165, 166, 175, 176, 177, 178, 179, 180, 182, 184, 186, 193, 197, 198, 208, 215, 220, 224, 225, 226, 229, 234, 235, 237, 238, 243, 244, 245, 246, 251, 252, 254, 255, 256, 257, 258, 260, 263, 264, 265, 267, 268, 269, 271, 272, 275, 276, 277, 280, 281, 282, 283, 292, 305, 306, 307, 309, 311, 314, 316, 319, 322, 323, 326, 330, 333, 334, 335, 337, 341, 343, 344, 349, 352, 354, 355, 356, 361, 362, 363, 370, 373, 375, 376, 377, 380, 384, 386, 387, 392, 395, 397, 399, 400, 401, 407, 409, 410, 411, 418, 419, 420, 421, 424, 425, 427, 428, 431, 438, 441, 444, 449, 458, 460, 462, 463, 464, 465, 466, 467, 468, 475, 476, 477, 482, 485, 486, 488, 489, 490, 491, 495, 497, 498, 499, 500, 502, 503, 504, 506, 507, 508, 509, 510, 511, 512, 513, 514, 517, 518, 521, 522, 523, 525, 529, 532, 534, 535, 536, 537, 539, 540, 543, 549, 551, 552, 554, 555, 556, 558, 560, 564, 568, 570, 573, 575, 576, 577, 578, 579, 581, 583, 585, 586, 588, 589, 591, 593, 594, 596, 598, 600, 603, 604, 605, 609, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 628, 629, 630, 633, 635, 636, 641, 644, 645, 646, 647, 648, 651, 653, 654, 656, 657, 658, 659, 660, 661, 663, 664, 665, 671, 675, 677, 680, 681, 683, 684, 686, 687, 689, 690, 691, 692, 693, 694, 695, 696, 697, 700, 702, 709, 710, 711, 719, 724, 728, 730, 732, 733, 734, 735, 736, 742, 743, 744, 745, 746, 748, 749, 753, 755, 756, 757, 760, 767, 768, 769, 770, 773, 776, 787, 791, 792, 793, 796, 800, 807, 809, 811, 816, 818, 819, 820, 822, 826, 828, 831, 833, 836, 840, 848, 850, 854, 868, 870, 871, 872, 873, 874, 875, 877, 880, 892, 893, 895, 896, 897, 898, 903, 905, 906, 908, 912, 913, 915, 919, 923, 928, 929, 932, 933, 934, 935, 937, 938, 939, 941, 944, 945, 947, 949, 950, 952, 953, 954, 958, 959, 960, 961, 962, 967, 968, 969, 972, 974, 975, 977, 978, 979, 981, 982, 983, 984, 985, 987, 989, 993, 994, 995]\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for k in ks:\n",
    "    for n in ns:\n",
    "        print(f\"BC2 with k={k} n={n}\")\n",
    "        evaluate_benchmark(\n",
    "            content_benchmark,\n",
    "            \"content\",\n",
    "            k,\n",
    "            collection,\n",
    "            retriever,\n",
    "            stemmer,\n",
    "            n=n,\n",
    "            use_rephrased_questions=True,\n",
    "            use_reranker=use_reranker,\n",
    "        )\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k in ks:\n",
    "#     for n in ns:\n",
    "#         print(f\"BX1 with k={k} n={n}\")\n",
    "#         evaluate_benchmark(\n",
    "#             context_benchmark,\n",
    "#             \"context\",\n",
    "#             k,\n",
    "#             collection,\n",
    "#             retriever,\n",
    "#             stemmer,\n",
    "#             n=n,\n",
    "#             use_reranker=use_reranker,\n",
    "#         )\n",
    "#         print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k in ks:\n",
    "#     for n in ns:\n",
    "#         print(f\"BX2 with k={k} n={n}\")\n",
    "#         evaluate_benchmark(\n",
    "#             context_benchmark,\n",
    "#             \"context\",\n",
    "#             k,\n",
    "#             collection,\n",
    "#             retriever,\n",
    "#             stemmer,\n",
    "#             n=n,\n",
    "#             use_rephrased_questions=True,\n",
    "#             use_reranker=use_reranker,\n",
    "#         )\n",
    "#         print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summary-neo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
