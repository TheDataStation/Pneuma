{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import chromadb\n",
    "import numpy as np\n",
    "import setproctitle\n",
    "\n",
    "setproctitle.setproctitle(\"python3.12\")\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "from chromadb.api.models.Collection import Collection\n",
    "from benchmark_generator.context.utils.jsonl import read_jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"fetaqa\"\n",
    "\n",
    "if dataset == \"chicago\":\n",
    "    content_benchmark = read_jsonl(\"../../data_src/benchmarks/content/pneuma_chicago_10K_questions_annotated.jsonl\")\n",
    "    context_benchmark = read_jsonl(\"../../data_src/benchmarks/context/chicago/bx_chicago.jsonl\")\n",
    "elif dataset == \"public\":\n",
    "    content_benchmark = read_jsonl(\"../../data_src/benchmarks/content/pneuma_public_bi_questions_annotated.jsonl\")\n",
    "    context_benchmark = read_jsonl(\"../../data_src/benchmarks/context/public/bx_public.jsonl\")\n",
    "elif dataset == \"chembl\":\n",
    "    content_benchmark = read_jsonl(\"../../data_src/benchmarks/content/pneuma_chembl_10K_questions_annotated.jsonl\")\n",
    "    context_benchmark = read_jsonl(\"../../data_src/benchmarks/context/chembl/bx_chembl.jsonl\")\n",
    "elif dataset == \"adventure\":\n",
    "    content_benchmark = read_jsonl(\"../../data_src/benchmarks/content/pneuma_adventure_works_questions_annotated.jsonl\")\n",
    "    context_benchmark = read_jsonl(\"../../data_src/benchmarks/context/adventure/bx_adventure.jsonl\")\n",
    "elif dataset == \"fetaqa\":\n",
    "    content_benchmark = read_jsonl(\"../../data_src/benchmarks/content/pneuma_fetaqa_questions_annotated.jsonl\")\n",
    "    context_benchmark = read_jsonl(\"../../data_src/benchmarks/context/fetaqa/bx_fetaqa.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "client = chromadb.PersistentClient(f\"../indices/index-{dataset}-pneuma-summarizer\")\n",
    "collection = client.get_collection(\"benchmark\")\n",
    "end = time.time()\n",
    "print(f\"Indexing time: {end-start} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_benchmark(\n",
    "    benchmark: list[dict[str,str]],\n",
    "    benchmark_type: str,\n",
    "    k: int,\n",
    "    collection: Collection,\n",
    "    use_rephrased_questions=False\n",
    "):\n",
    "    start = time.time()\n",
    "    hitrate_sum = 0\n",
    "    wrong_questions = []\n",
    "    def get_question_key(benchmark_type: str):\n",
    "        if benchmark_type == \"content\":\n",
    "            if not use_rephrased_questions:\n",
    "                question_key = \"question_from_sql_1\"\n",
    "            else:\n",
    "                question_key = \"question\"\n",
    "        else:\n",
    "            if not use_rephrased_questions:\n",
    "                question_key = \"question_bx1\"\n",
    "            else:\n",
    "                question_key = \"question_bx2\"\n",
    "        return question_key\n",
    "    question_key = get_question_key(benchmark_type)\n",
    "\n",
    "    questions = []\n",
    "    for data in benchmark:\n",
    "        questions.append(data[question_key])\n",
    "    embed_questions = np.loadtxt(\n",
    "        f\"../embeddings/embed-{dataset}-questions-{benchmark_type}-{use_rephrased_questions}.txt\"\n",
    "    )\n",
    "    embed_questions = [embed.tolist() for embed in embed_questions]\n",
    "\n",
    "    for idx, datum in enumerate(tqdm(benchmark)):\n",
    "        answer_tables = datum[\"answer_tables\"]\n",
    "        vec_res = collection.query(\n",
    "            query_embeddings=[embed_questions[idx]],\n",
    "            n_results=k\n",
    "        )\n",
    "        before = hitrate_sum\n",
    "        for res in vec_res['ids'][0]:\n",
    "            table = res.split(\"_SEP_\")[0]\n",
    "            if table in answer_tables:\n",
    "                hitrate_sum += 1\n",
    "                break\n",
    "        if before == hitrate_sum:\n",
    "            wrong_questions.append(idx)\n",
    "    \n",
    "    end = time.time()\n",
    "\n",
    "    print(f\"Hit Rate: {hitrate_sum/len(benchmark) * 100}\")\n",
    "    print(f\"Benchmarking Time: {end - start} seconds\")\n",
    "    print(f\"Wrongly answered questions: {wrong_questions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BC1\n",
    "evaluate_benchmark(\n",
    "    content_benchmark, \"content\", 1, collection\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BC2\n",
    "evaluate_benchmark(\n",
    "    content_benchmark, \"content\", 1, collection, use_rephrased_questions=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BX1\n",
    "evaluate_benchmark(\n",
    "    context_benchmark, \"context\", 1, collection\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BX2\n",
    "evaluate_benchmark(\n",
    "    context_benchmark, \"context\", 1, collection, use_rephrased_questions=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summary-neo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
