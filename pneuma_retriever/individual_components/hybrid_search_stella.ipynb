{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/p-storage/luthfibalaka_d2b65c57/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import setproctitle\n",
    "\n",
    "setproctitle.setproctitle(\"python3\")\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import chromadb\n",
    "import bm25s\n",
    "import Stemmer\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import set_seed\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from chromadb.api.models.Collection import Collection\n",
    "from benchmark_generator.context.utils.jsonl import read_jsonl\n",
    "\n",
    "\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "set_seed(42, deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = Stemmer.Stemmer(\"english\")\n",
    "reranker = SentenceTransformer(\"../../models/stella\", local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"fetaqa\"\n",
    "if dataset == \"chicago\":\n",
    "    narrations = read_jsonl(\"../../pneuma_summarizer/summaries/narrations/chicago_narrations.jsonl\")\n",
    "    rows = read_jsonl(\"../../pneuma_summarizer/summaries/rows/chicago.jsonl\")\n",
    "    content_benchmark = read_jsonl(\"../../data_src/benchmarks/content/pneuma_chicago_10K_questions_annotated.jsonl\")\n",
    "    path = \"../../data_src/tables/pneuma_chicago_10K\"\n",
    "elif dataset == \"public\":\n",
    "    narrations = read_jsonl(\"../../pneuma_summarizer/summaries/narrations/public_narrations.jsonl\")\n",
    "    rows = read_jsonl(\"../../pneuma_summarizer/summaries/rows/public.jsonl\")\n",
    "    content_benchmark = read_jsonl(\"../../data_src/benchmarks/content/pneuma_public_bi_questions_annotated.jsonl\")\n",
    "    path = \"../../data_src/tables/pneuma_public_bi\"\n",
    "elif dataset == \"chembl\":\n",
    "    narrations = read_jsonl(\"../../pneuma_summarizer/summaries/narrations/chembl_narrations.jsonl\")\n",
    "    rows = read_jsonl(\"../../pneuma_summarizer/summaries/rows/chembl.jsonl\")\n",
    "    content_benchmark = read_jsonl(\"../../data_src/benchmarks/content/pneuma_chembl_10K_questions_annotated.jsonl\")\n",
    "    path = \"../../data_src/tables/pneuma_chembl_10K\"\n",
    "elif dataset == \"adventure\":\n",
    "    narrations = read_jsonl(\"../../pneuma_summarizer/summaries/narrations/adventure_narrations.jsonl\")\n",
    "    rows = read_jsonl(\"../../pneuma_summarizer/summaries/rows/adventure.jsonl\")\n",
    "    content_benchmark = read_jsonl(\"../../data_src/benchmarks/content/pneuma_adventure_works_questions_annotated.jsonl\")\n",
    "    path = \"../../data_src/tables/pneuma_adventure_works\"\n",
    "elif dataset == \"fetaqa\":\n",
    "    narrations = read_jsonl(\"../../pneuma_summarizer/summaries/narrations/fetaqa_narrations.jsonl\")\n",
    "    rows = read_jsonl(\"../../pneuma_summarizer/summaries/rows/fetaqa.jsonl\")\n",
    "    content_benchmark = read_jsonl(\"../../data_src/benchmarks/content/pneuma_fetaqa_questions_annotated.jsonl\")\n",
    "    path = \"../../data_src/tables/pneuma_fetaqa\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexing_keyword(\n",
    "    stemmer,\n",
    "    narration_contents: list[dict[str, str]],\n",
    "    contexts: list[dict[str, str]] = None,\n",
    "):\n",
    "    corpus_json = []\n",
    "    tables = sorted({content[\"table\"] for content in narration_contents})\n",
    "    for table in tables:\n",
    "        cols_descriptions = [\n",
    "            content[\"summary\"]\n",
    "            for content in narration_contents\n",
    "            if content[\"table\"] == table\n",
    "        ]\n",
    "        for content_idx, content in enumerate(cols_descriptions):\n",
    "            corpus_json.append(\n",
    "                {\n",
    "                    \"text\": content,\n",
    "                    \"metadata\": {\"table\": f\"{table}_SEP_contents_{content_idx}\"},\n",
    "                }\n",
    "            )\n",
    "\n",
    "        if contexts is not None:\n",
    "            filtered_contexts = [\n",
    "                context[\"context\"] for context in contexts if context[\"table\"] == table\n",
    "            ]\n",
    "            for context_idx, context in enumerate(filtered_contexts):\n",
    "                corpus_json.append(\n",
    "                    {\n",
    "                        \"text\": context,\n",
    "                        \"metadata\": {\"table\": f\"{table}_SEP_{context_idx}\"},\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    corpus_text = [doc[\"text\"] for doc in corpus_json]\n",
    "    corpus_tokens = bm25s.tokenize(\n",
    "        corpus_text, stopwords=\"en\", stemmer=stemmer, show_progress=False\n",
    "    )\n",
    "\n",
    "    retriever = bm25s.BM25(corpus=corpus_json)\n",
    "    retriever.index(corpus_tokens, show_progress=False)\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fetaqa dataset\n"
     ]
    }
   ],
   "source": [
    "print(f\"Processing {dataset} dataset\")\n",
    "start = time.time()\n",
    "client = chromadb.PersistentClient(\n",
    "    f\"../indices/index-{dataset}-content-rows-narrations\"\n",
    ")\n",
    "collection = client.get_collection(\"benchmark\")\n",
    "retriever = indexing_keyword(stemmer, rows + narrations)\n",
    "end = time.time()\n",
    "print(f\"Indexing time: {end-start} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_nodes_bm25(items):\n",
    "    # Normalize relevance scores and return the nodes in dict format\n",
    "    results, scores = items\n",
    "    scores: list[float] = scores[0]\n",
    "    max_score = max(scores)\n",
    "    min_score = min(scores)\n",
    "\n",
    "    processed_nodes: dict[str, tuple[float, str]] = {}\n",
    "    for i, node in enumerate(results[0]):\n",
    "        if min_score == max_score:\n",
    "            score = 1\n",
    "        else:\n",
    "            score = (scores[i] - min_score) / (max_score - min_score)\n",
    "        processed_nodes[node[\"metadata\"][\"table\"]] = (score, node[\"text\"])\n",
    "    return processed_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_nodes_vec(items):\n",
    "    # Normalize relevance scores and return the nodes in dict format\n",
    "    scores: list[float] = [1 - dist for dist in items[\"distances\"][0]]\n",
    "    max_score = max(scores)\n",
    "    min_score = min(scores)\n",
    "\n",
    "    processed_nodes: dict[str, tuple[float, str]] = {}\n",
    "\n",
    "    for idx in range(len(items[\"ids\"][0])):\n",
    "        if min_score == max_score:\n",
    "            score = 1\n",
    "        else:\n",
    "            score = (scores[idx] - min_score) / (max_score - min_score)\n",
    "        processed_nodes[items[\"ids\"][0][idx]] = (score, items[\"documents\"][0][idx])\n",
    "    return processed_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_retriever(\n",
    "    bm25_res,\n",
    "    vec_res,\n",
    "    k: int,\n",
    "    question: str,\n",
    "    use_reranker=False,\n",
    "):\n",
    "    processed_nodes_bm25 = process_nodes_bm25(bm25_res)\n",
    "    processed_nodes_vec = process_nodes_vec(vec_res)\n",
    "\n",
    "    node_ids = set(list(processed_nodes_bm25.keys()) + list(processed_nodes_vec.keys()))\n",
    "    all_nodes: list[tuple[str, float, str]] = []\n",
    "    for node_id in sorted(node_ids):\n",
    "        bm25_score_doc = processed_nodes_bm25.get(node_id, (0.0, None))\n",
    "        vec_score_doc = processed_nodes_vec.get(node_id, (0.0, None))\n",
    "\n",
    "        combined_score = 0.5 * bm25_score_doc[0] + 0.5 * vec_score_doc[0]\n",
    "        if bm25_score_doc[1] is None:\n",
    "            doc = vec_score_doc[1]\n",
    "        else:\n",
    "            doc = bm25_score_doc[1]\n",
    "\n",
    "        all_nodes.append((node_id, combined_score, doc))\n",
    "\n",
    "    sorted_nodes = sorted(all_nodes, key=lambda node: (-node[1], node[0]))[:k]\n",
    "    if use_reranker:\n",
    "        sorted_nodes = rerank(sorted_nodes, question)\n",
    "    return sorted_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "def rerank(nodes: list[tuple[str, float, str]], question: str):\n",
    "    # Each node is of the form (name, score, doc)\n",
    "    names = []\n",
    "    docs = []\n",
    "    for node in nodes:\n",
    "        names.append(node[0])\n",
    "        docs.append(node[2])\n",
    "\n",
    "    docs_embeddings = reranker.encode(\n",
    "        docs,\n",
    "        batch_size=100,\n",
    "        device=\"cuda\",\n",
    "    )\n",
    "    question_embedding = reranker.encode(question, device=\"cuda\")\n",
    "    similarities = [\n",
    "        1 - cosine(question_embedding, docs_embedding) for docs_embedding in docs_embeddings\n",
    "    ]\n",
    "\n",
    "    reranked_nodes = sorted(zip(names, similarities, docs), key=lambda x: (-x[1], x[0]))\n",
    "    return reranked_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_question_key(benchmark_type: str, use_rephrased_questions: bool):\n",
    "    if benchmark_type == \"content\":\n",
    "        if not use_rephrased_questions:\n",
    "            question_key = \"question_from_sql_1\"\n",
    "        else:\n",
    "            question_key = \"question\"\n",
    "    else:\n",
    "        if not use_rephrased_questions:\n",
    "            question_key = \"question_bx1\"\n",
    "        else:\n",
    "            question_key = \"question_bx2\"\n",
    "    return question_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_benchmark(\n",
    "    benchmark: list[dict[str, str]],\n",
    "    benchmark_type: str,\n",
    "    k: int,\n",
    "    collection: Collection,\n",
    "    retriever,\n",
    "    stemmer,\n",
    "    n=3,\n",
    "    use_reranker=False,\n",
    "    use_rephrased_questions=False,\n",
    "):\n",
    "    start = time.time()\n",
    "    hitrate_sum = 0\n",
    "    wrong_questions = []\n",
    "\n",
    "    if use_reranker:\n",
    "        increased_k = k * n\n",
    "    else:\n",
    "        increased_k = k * n\n",
    "\n",
    "    question_key = get_question_key(benchmark_type, use_rephrased_questions)\n",
    "\n",
    "    questions = []\n",
    "    for data in benchmark:\n",
    "        questions.append(data[question_key])\n",
    "    embed_questions = np.loadtxt(\n",
    "        f\"../embeddings/embed-{dataset}-questions-{benchmark_type}-{use_rephrased_questions}.txt\"\n",
    "    )\n",
    "    embed_questions = [embed.tolist() for embed in embed_questions]\n",
    "\n",
    "    for idx, datum in enumerate(tqdm(benchmark)):\n",
    "        answer_tables = datum[\"answer_tables\"]\n",
    "        question_embedding = embed_questions[idx]\n",
    "\n",
    "        query_tokens = bm25s.tokenize(\n",
    "            questions[idx], stemmer=stemmer, show_progress=False\n",
    "        )\n",
    "        results, scores = retriever.retrieve(\n",
    "            query_tokens, k=increased_k, show_progress=False\n",
    "        )\n",
    "        bm25_res = (results, scores)\n",
    "\n",
    "        vec_res = collection.query(\n",
    "            query_embeddings=[question_embedding], n_results=increased_k\n",
    "        )\n",
    "\n",
    "        all_nodes = hybrid_retriever(\n",
    "            bm25_res, vec_res, increased_k, questions[idx], use_reranker\n",
    "        )\n",
    "        before = hitrate_sum\n",
    "        for table, _, _ in all_nodes[:k]:\n",
    "            table = table.split(\"_SEP_\")[0]\n",
    "            if table in answer_tables:\n",
    "                hitrate_sum += 1\n",
    "                break\n",
    "        if before == hitrate_sum:\n",
    "            wrong_questions.append(idx)\n",
    "        # Checkpoint\n",
    "        if idx % 20 == 0:\n",
    "            print(f\"Current Hit Rate Sum at index {idx}: {hitrate_sum}\")\n",
    "            print(\n",
    "                f\"Current wrongly answered questions at index {idx}: {wrong_questions}\"\n",
    "            )\n",
    "\n",
    "    end = time.time()\n",
    "    print(f\"Hit Rate: {round(hitrate_sum/len(benchmark) * 100, 2)}\")\n",
    "    print(f\"Benchmarking Time: {end - start} seconds\")\n",
    "    print(f\"Wrongly answered questions: {wrong_questions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = [1]\n",
    "ns = [5]\n",
    "use_reranker = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in ks:\n",
    "    for n in ns:\n",
    "        print(f\"BC1 with k={k} n={n}\")\n",
    "        evaluate_benchmark(\n",
    "            content_benchmark,\n",
    "            \"content\",\n",
    "            k,\n",
    "            collection,\n",
    "            retriever,\n",
    "            stemmer,\n",
    "            n=n,\n",
    "            use_reranker=use_reranker,\n",
    "        )\n",
    "        print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in ks:\n",
    "    for n in ns:\n",
    "        print(f\"BC2 with k={k} n={n}\")\n",
    "        evaluate_benchmark(\n",
    "            content_benchmark,\n",
    "            \"content\",\n",
    "            k,\n",
    "            collection,\n",
    "            retriever,\n",
    "            stemmer,\n",
    "            n=n,\n",
    "            use_rephrased_questions=True,\n",
    "            use_reranker=use_reranker,\n",
    "        )\n",
    "    print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summary-neo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
