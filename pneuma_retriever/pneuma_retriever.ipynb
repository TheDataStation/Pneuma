{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import setproctitle\n",
    "import json\n",
    "import chromadb\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "import bm25s\n",
    "import Stemmer\n",
    "\n",
    "from transformers import set_seed\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "setproctitle.setproctitle(\"python\")\n",
    "set_seed(42, deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "def read_jsonl(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            data.append(json.loads(line.strip()))\n",
    "    return data\n",
    "model = SentenceTransformer('dunzhang/stella_en_1.5B_v5', trust_remote_code=True)\n",
    "stemmer = Stemmer.Stemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"public\"\n",
    "if dataset == \"chicago\":\n",
    "    col_descriptions = pd.read_csv(\"../pneuma_summarizer_narrations/chicago_cols.csv\")\n",
    "    jsonl_data = read_jsonl(\"benchmarks/content/pneuma_chicago_10K_questions_annotated.jsonl\")\n",
    "    # context_benchmark = pd.read_csv(\"benchmarks/BX1_chicago_corrected.csv\")\n",
    "    contexts = pd.read_csv(\"contexts/contexts_chicago.csv\")\n",
    "    path = \"datasets/pneuma_chicago_10K\"\n",
    "elif dataset == \"public\":\n",
    "    col_descriptions = pd.read_csv(\"narrations/public_cols.csv\")\n",
    "    jsonl_data = read_jsonl(\"benchmarks/content/pneuma_public_bi_questions_annotated.jsonl\")\n",
    "    # context_benchmark = pd.read_csv(\"benchmarks/BX1_public_corrected.csv\")\n",
    "    contexts = pd.read_csv(\"contexts/contexts_public.csv\")\n",
    "    path = \"datasets/public_bi_benchmark\"\n",
    "elif dataset == \"chembl\":\n",
    "    col_descriptions = pd.read_csv(\"narrations/chembl_cols.csv\")\n",
    "    jsonl_data = read_jsonl(\"benchmarks/content/pneuma_chembl_10K_questions_annotated.jsonl\")\n",
    "    # context_benchmark = pd.read_csv(\"\")\n",
    "    contexts = pd.read_csv(\"contexts/contexts_chembl.csv\")\n",
    "    path = \"datasets/pneuma_chembl_10K\"\n",
    "\n",
    "con = duckdb.connect()\n",
    "tables = [file[:-4] for file in sorted(os.listdir(path)) if file.endswith(\".csv\")]\n",
    "tables.sort()\n",
    "\n",
    "import shutil\n",
    "try:\n",
    "    shutil.rmtree(f\"experiment-{dataset}\")\n",
    "except:\n",
    "    pass\n",
    "client = chromadb.PersistentClient(f\"experiment-{dataset}\")\n",
    "collection = client.create_collection(name=\"benchmark\", metadata={\"hnsw:space\": \"cosine\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processed_df(path: str, table: str) -> pd.DataFrame:\n",
    "    df = con.sql(f\"from '{path}/{table}.csv'\").to_df().drop_duplicates().reset_index(drop=True)\n",
    "    for col in df.columns:\n",
    "        if pd.api.types.is_datetime64_any_dtype(df[col]):\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "            df[col] = df[col].apply(\n",
    "                lambda x: x.strftime('%B ') + str(x.day).lstrip('0') + x.strftime(', %Y %H:%M:%S.%f')[:-3] if pd.notnull(x) else 'NaT'\n",
    "            )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_largest_smaller_or_equal(tokens_list: list[int], max_tokens: int):\n",
    "    for idx in range(len(tokens_list) - 1, -1, -1):\n",
    "        if tokens_list[idx] <= max_tokens:\n",
    "            return idx\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevancy_prompt(dataset: str, question: str):\n",
    "    return f\"\"\"Given this dataset:\n",
    "*/\n",
    "{dataset}\n",
    "*/\n",
    "and this question:\n",
    "/*\n",
    "{question}\n",
    "*/\n",
    "Is the dataset relevant to answer the question? Begin your answer with yes/no.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.pipeline_initializer import initialize_pipeline\n",
    "from utils.prompting_interface import prompt_pipeline\n",
    "import torch\n",
    "\n",
    "pipe = initialize_pipeline(\"meta-llama/Meta-Llama-3-8B-Instruct\", torch.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def indexing_vector(model, collection, tables: list[str], contexts: pd.DataFrame):\n",
    "    embeddings = []\n",
    "    metadatas = []\n",
    "    ids = []\n",
    "\n",
    "    for table in tqdm(tables):\n",
    "        df = con.sql(f\"select * from '{path}/{table}.csv'\").to_df().drop_duplicates().reset_index(drop=True)\n",
    "        columns = \" | \".join(df.columns)\n",
    "        col_embedding = model.encode(columns)\n",
    "\n",
    "        embeddings.append(col_embedding.tolist())\n",
    "        metadatas.append({\"table\": f\"{table}_SEP_columns\"})\n",
    "        ids.append(f\"{table}_columns\")\n",
    "\n",
    "        table_contexts = contexts[contexts[\"table\"].str.endswith(table)].reset_index(drop=True)\n",
    "        for ctx_idx, context in enumerate(table_contexts[\"context\"]):\n",
    "            embedding = model.encode(context)\n",
    "            embeddings.append(embedding.tolist())\n",
    "            metadatas.append({\"table\": f\"{table}_SEP_{ctx_idx}\"})\n",
    "            ids.append(f\"{table}_context_{ctx_idx}\")\n",
    "\n",
    "    iterations = math.ceil(len(embeddings) / collection_max_batch_size)\n",
    "\n",
    "    for i in tqdm(range(iterations), \"inserting into collections\"):\n",
    "        lower_bound = (i * collection_max_batch_size) % len(embeddings)\n",
    "        upper_bound = min(len(embeddings), (lower_bound + collection_max_batch_size) % len(embeddings))\n",
    "\n",
    "        if upper_bound > lower_bound:\n",
    "            embeddings_batch = embeddings[lower_bound:upper_bound]\n",
    "            metadatas_batch = metadatas[lower_bound:upper_bound]\n",
    "            ids_batch = ids[lower_bound:upper_bound]\n",
    "        else:\n",
    "            embeddings_batch = embeddings[lower_bound:] + embeddings[:upper_bound]\n",
    "            metadatas_batch = metadatas[lower_bound:] + metadatas[:upper_bound]\n",
    "            ids_batch = ids[lower_bound:] + ids[:upper_bound]\n",
    "\n",
    "        collection.add(\n",
    "            embeddings=embeddings_batch,\n",
    "            metadatas=metadatas_batch,\n",
    "            ids=ids_batch\n",
    "        )\n",
    "\n",
    "def indexing_keyword(stemmer, col_descriptions: str, tables: list[str], contexts: pd.DataFrame):\n",
    "    corpus_json = []\n",
    "\n",
    "    for table in tqdm(tables):\n",
    "        filtered_col_descriptions = col_descriptions[col_descriptions[\"table\"] == table].reset_index(drop=True)\n",
    "        col_description = filtered_col_descriptions[\"description\"].to_list()\n",
    "        col_summary = \" | \".join(col_description)\n",
    "\n",
    "        corpus_json.append({\"text\": col_summary, \"metadata\": {\"table\": f\"{table}_SEP_columns\"}})\n",
    "\n",
    "        table_contexts = contexts[contexts[\"table\"].str.endswith(table)].reset_index(drop=True)\n",
    "        for context_idx, context in enumerate(table_contexts[\"context\"]):\n",
    "            corpus_json.append({\"text\": context, \"metadata\": {\"table\": f\"{table}_SEP_{context_idx}\"}})\n",
    "\n",
    "    corpus_text = [doc[\"text\"] for doc in corpus_json]\n",
    "    corpus_tokens = bm25s.tokenize(corpus_text, stopwords=\"en\", stemmer=stemmer, show_progress=False)\n",
    "\n",
    "    retriever = bm25s.BM25(corpus=corpus_json)\n",
    "    retriever.index(corpus_tokens, show_progress=False)\n",
    "\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexing_vector(model, collection, tables, contexts)\n",
    "retriever = indexing_keyword(stemmer, col_descriptions, tables, contexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_table_relevant(\n",
    "    path: str,\n",
    "    table: str,\n",
    "    max_tokens: int,\n",
    "    question: str\n",
    ") -> bool:\n",
    "    df = get_processed_df(path, table)\n",
    "    columns = \"col: \" + \" | \".join(df.columns)\n",
    "    rows = [\"\"] * len(df)\n",
    "\n",
    "    required_tokens = [len(pipe.tokenizer.tokenize(columns))] * len(df)\n",
    "\n",
    "    for row_idx, row in df.iterrows():\n",
    "        rows[row_idx] = f\"row {row_idx+1}: \" + \" | \".join(row.astype(str))\n",
    "        required_tokens[row_idx] += len(pipe.tokenizer.tokenize(rows[row_idx]))\n",
    "        if row_idx > 0:\n",
    "            required_tokens[row_idx] += required_tokens[row_idx-1]\n",
    "\n",
    "    last_processed_idx = 0\n",
    "    while last_processed_idx < len(required_tokens):\n",
    "\n",
    "        to_process_idx = find_largest_smaller_or_equal(required_tokens[last_processed_idx:], max_tokens)\n",
    "        if to_process_idx == -1:\n",
    "            return False\n",
    "\n",
    "        to_process_idx += last_processed_idx\n",
    "        prompt = get_relevancy_prompt(\n",
    "            columns + \"\\n\" + \"\\n\".join(rows[last_processed_idx:to_process_idx+1]),\n",
    "            question\n",
    "        )\n",
    "\n",
    "        answer: str = prompt_pipeline(\n",
    "            pipe, [{\"role\": \"user\", \"content\": prompt}], context_length=8192, max_new_tokens=3, top_p=None, temperature=None\n",
    "        )[-1][\"content\"]\n",
    "\n",
    "        if answer.lower().startswith(\"yes\"):\n",
    "            return True\n",
    "\n",
    "        last_processed_idx = to_process_idx + 1\n",
    "        for i in range(len(required_tokens[last_processed_idx:])):\n",
    "            required_tokens[last_processed_idx+i] -= (required_tokens[last_processed_idx-1] - len(pipe.tokenizer.tokenize(columns)))\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def rerank(nodes: list[tuple], question: str):\n",
    "    tables_relevancy = defaultdict(bool)\n",
    "\n",
    "    for node in nodes:\n",
    "        table_name = node[0]\n",
    "        if is_table_relevant(path, table_name, 7000, question):\n",
    "            tables_relevancy[table_name] = True\n",
    "    new_nodes = [(table_name, score) for table_name, score in nodes if tables_relevancy[table_name]] + [(table_name, score) for table_name, score in nodes if not tables_relevancy[table_name]]\n",
    "    return new_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_nodes_bm25(results, scores):\n",
    "    # Normalize relevance scores and return the nodes in dict format.\n",
    "    scores: list[float] = scores[0]\n",
    "    max_score = max(scores)\n",
    "    min_score = min(scores)\n",
    "\n",
    "    processed_nodes = {}\n",
    "    for i, node in enumerate(results[0]):\n",
    "        if min_score == max_score:\n",
    "            score = 1\n",
    "        else:\n",
    "            score = (scores[i] - min_score) / (max_score - min_score)\n",
    "        processed_nodes[node[\"metadata\"][\"table\"]] = score\n",
    "    return processed_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_nodes_vec(items):\n",
    "    # Normalize relevance scores and return the nodes in dict format.\n",
    "    scores: list[float] = [1 - dist for dist in items[\"distances\"][0]]\n",
    "    max_score = max(scores)\n",
    "    min_score = min(scores)\n",
    "\n",
    "    processed_nodes = {}\n",
    "    for idx, table in enumerate(items['metadatas'][0]):\n",
    "        if min_score == max_score:\n",
    "            score = 1\n",
    "        else:\n",
    "            score = (scores[idx] - min_score) / (max_score - min_score)\n",
    "        processed_nodes[table[\"table\"]] = score\n",
    "    return processed_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(bm25_res, bm25_sc, vec_res, k: int, question: str, ranking=False):\n",
    "    processed_nodes_bm25 = process_nodes_bm25(bm25_res, bm25_sc)\n",
    "    processed_nodes_vec: dict = process_nodes_vec(vec_res)\n",
    "\n",
    "    node_ids = set(list(processed_nodes_bm25.keys()) + list(processed_nodes_vec.keys()))\n",
    "    all_nodes = []\n",
    "    for node_id in node_ids:\n",
    "        try:\n",
    "            bm25_score = processed_nodes_bm25.get(node_id, 0.0)\n",
    "        except:\n",
    "            bm25_score = 0.0\n",
    "        try:\n",
    "            cosine_score = processed_nodes_vec.get(node_id, 0.0)\n",
    "        except:\n",
    "            cosine_score = 0.0\n",
    "        combined_score = 0.5 * bm25_score + 0.5 * cosine_score\n",
    "        all_nodes.append((node_id, combined_score))\n",
    "    \n",
    "    sorted_nodes = sorted(all_nodes, key=lambda node: (-node[1], node[0]))[:k]\n",
    "    if ranking:\n",
    "        reranked_nodes = rerank(sorted_nodes, question)\n",
    "        return reranked_nodes\n",
    "    return sorted_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(jsonl_data, k, model, collection, retriever, stemmer, ranking=False, rephrase=False):\n",
    "    hitrate_sum = 0\n",
    "    wrong_list = []\n",
    "\n",
    "    if ranking:\n",
    "        increased_k = k * 3\n",
    "    else:\n",
    "        increased_k = k\n",
    "    \n",
    "    if not rephrase:\n",
    "        question_key = \"question_from_sql_1\"\n",
    "    else:\n",
    "        question_key = \"question\"\n",
    "\n",
    "    for idx, datum in enumerate(tqdm(jsonl_data)):\n",
    "        answer_tables = datum[\"answer_tables\"]\n",
    "        question_embedding = model.encode(datum[question_key]).tolist()\n",
    "\n",
    "        query_tokens = bm25s.tokenize(datum[question_key], stemmer=stemmer, show_progress=False)\n",
    "        results, scores = retriever.retrieve(query_tokens, k=increased_k, show_progress=False)\n",
    "\n",
    "        vec_res = collection.query(\n",
    "            query_embeddings=[question_embedding],\n",
    "            n_results=increased_k\n",
    "        )\n",
    "\n",
    "        all_nodes = process(results, scores, vec_res, increased_k, datum[question_key], ranking)\n",
    "        before = hitrate_sum\n",
    "        for table, _ in all_nodes[:k]:\n",
    "            table = table.split(\"_SEP_\")[0]\n",
    "            if table in answer_tables:\n",
    "                hitrate_sum += 1\n",
    "                break\n",
    "        if before == hitrate_sum:\n",
    "            wrong_list.append(idx)\n",
    "        # Checkpoint\n",
    "        # if idx % 25 == 0:\n",
    "        #     print(f\"Current Hit Rate Sum: {hitrate_sum}\")\n",
    "    print(f\"Final Hit Rate Sum: {hitrate_sum}\")\n",
    "    print(f\"Hit Rate: {hitrate_sum/len(jsonl_data)}\")\n",
    "    print(f\"Wrong List: {wrong_list}\")\n",
    "    return hitrate_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = [1, 10, 30, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in ks:\n",
    "    print(f\"K (no rerank): {k}\")\n",
    "    result = benchmark(jsonl_data, k, model, collection, retriever, stemmer, rephrase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in ks:\n",
    "    print(f\"K (no rerank): {k}\")\n",
    "    result = benchmark(jsonl_data, k, model, collection, retriever, stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_context(benchmark: pd.DataFrame, k, model, collection, retriever, stemmer, ranking=False):\n",
    "    hitrate_sum = 0\n",
    "    wrong_list = []\n",
    "\n",
    "    if ranking:\n",
    "        increased_k = k * 3\n",
    "    else:\n",
    "        increased_k = k\n",
    "    \n",
    "    for row_idx, row in tqdm(benchmark.iterrows()):\n",
    "        answer_tables = row[\"answer_tables\"]\n",
    "        question_embedding = model.encode(row[\"question\"]).tolist()\n",
    "\n",
    "        query_tokens = bm25s.tokenize(row[\"question\"], stemmer=stemmer, show_progress=False)\n",
    "        results, scores = retriever.retrieve(query_tokens, k=increased_k, show_progress=False)\n",
    "\n",
    "        vec_res = collection.query(\n",
    "            query_embeddings=[question_embedding],\n",
    "            n_results=increased_k\n",
    "        )\n",
    "\n",
    "        all_nodes = process(results, scores, vec_res, increased_k, row[\"question\"], ranking)\n",
    "        before = hitrate_sum\n",
    "        for table, _ in all_nodes[:k]:\n",
    "            if table in answer_tables:\n",
    "                hitrate_sum += 1\n",
    "                break\n",
    "        if before == hitrate_sum:\n",
    "            wrong_list.append(row_idx)\n",
    "        # Checkpoint\n",
    "        # if idx % 25 == 0:\n",
    "        #     print(f\"Current Hit Rate Sum: {hitrate_sum}\")\n",
    "    print(f\"Final Hit Rate Sum: {hitrate_sum}\")\n",
    "    print(f\"Hit Rate: {hitrate_sum/len(benchmark)}\")\n",
    "    print(f\"Wrong List: {wrong_list}\")\n",
    "    return hitrate_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k in ks:\n",
    "#     print(f\"K (no rerank): {k}\")\n",
    "#     result = benchmark_context(context_benchmark, k, model, collection, retriever, stemmer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summary-neo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
