{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import setproctitle\n",
    "import chromadb\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "import bm25s\n",
    "import Stemmer\n",
    "import torch\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import set_seed\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from benchmark_generator.context.utils.pipeline_initializer import initialize_pipeline\n",
    "from benchmark_generator.context.utils.prompting_interface import prompt_pipeline\n",
    "\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "set_seed(42, deterministic=True)\n",
    "# Uncomment to select GPU\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# setproctitle.setproctitle(\"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.path.append(\"..\")\n",
    "# pipe = initialize_pipeline(\"meta-llama/Meta-Llama-3-8B-Instruct\", torch.bfloat16)\n",
    "# pipe.tokenizer.pad_token_id = pipe.model.config.eos_token_id\n",
    "# pipe.tokenizer.padding_side = 'left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsonl(file_path: str):\n",
    "    \"\"\"\n",
    "    Read a JSONL file\n",
    "    \"\"\"\n",
    "    data: list[dict[str, str]] = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            data.append(json.loads(line.strip()))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"chembl\"\n",
    "if dataset == \"chicago\":\n",
    "    std_contents = read_jsonl(\"../pneuma_summarizer/summaries/standard/chicago_standard.jsonl\")\n",
    "    narration_contents = read_jsonl(\"../pneuma_summarizer/summaries/narrations/chicago_narrations.jsonl\")\n",
    "    contexts = read_jsonl(\"../data_src/benchmarks/context/chicago/contexts_chicago.jsonl\")\n",
    "\n",
    "    content_benchmark = read_jsonl(\"../data_src/benchmarks/content/pneuma_chicago_10K_questions_annotated.jsonl\")\n",
    "    context_benchmark = read_jsonl(\"../data_src/benchmarks/context/chicago/bx_chicago.jsonl\")\n",
    "    path = \"../data_src/tables/pneuma_chicago_10K\"\n",
    "elif dataset == \"public\":\n",
    "    std_contents = read_jsonl(\"../pneuma_summarizer/summaries/standard/public_standard.jsonl\")\n",
    "    narration_contents = read_jsonl(\"../pneuma_summarizer/summaries/narrations/public_narrations.jsonl\")\n",
    "    contexts = read_jsonl(\"../data_src/benchmarks/context/public/contexts_public.jsonl\")\n",
    "\n",
    "    content_benchmark = read_jsonl(\"../data_src/benchmarks/content/pneuma_public_bi_questions_annotated.jsonl\")\n",
    "    context_benchmark = read_jsonl(\"../data_src/benchmarks/context/public/bx_public.jsonl\")\n",
    "    path = \"../data_src/tables/pneuma_public_bi\"\n",
    "elif dataset == \"chembl\":\n",
    "    std_contents = read_jsonl(\"../pneuma_summarizer/summaries/standard/chembl_standard.jsonl\")\n",
    "    narration_contents = read_jsonl(\"../pneuma_summarizer/summaries/narrations/chembl_narrations.jsonl\")\n",
    "    contexts = read_jsonl(\"../data_src/benchmarks/context/chembl/contexts_chembl.jsonl\")\n",
    "\n",
    "    content_benchmark = read_jsonl(\"../data_src/benchmarks/content/pneuma_chembl_10K_questions_annotated.jsonl\")\n",
    "    context_benchmark = read_jsonl(\"../data_src/benchmarks/context/chembl/bx_chembl.jsonl\")\n",
    "    path = \"../data_src/tables/pneuma_chembl_10K\"\n",
    "\n",
    "con = duckdb.connect()\n",
    "tables = [file[:-4] for file in sorted(os.listdir(path)) if file.endswith(\".csv\")]\n",
    "tables.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate IDs of contexts back into contexts\n",
    "ids_contexts = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexing_vector(\n",
    "    client,\n",
    "    embedding_model,\n",
    "    contexts: list[dict[str, str]],\n",
    "    std_contents: list[dict[str, str]],\n",
    "):\n",
    "    documents = []\n",
    "    metadatas = []\n",
    "    ids = []\n",
    "\n",
    "    try:\n",
    "        client.get_collection(\"benchmark\")\n",
    "        client.delete_collection(\"benchmark\")\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "\n",
    "    collection = client.create_collection(\n",
    "        name=\"benchmark\",\n",
    "        metadata={\"hnsw:space\": \"cosine\", \"hnsw:M\": 1024}\n",
    "    )\n",
    "\n",
    "    for content in tqdm(std_contents, \"Adding contents\"):\n",
    "        table = content[\"table\"]\n",
    "        documents.append(content[\"summary\"])\n",
    "        metadatas.append({\"table\": f\"{table}_SEP_contents\"})\n",
    "        ids.append(f\"{table}_SEP_contents\")\n",
    "\n",
    "        filtered_contexts = [context for context in contexts if context[\"table\"] == content[\"table\"]]\n",
    "        for context_idx, context in enumerate(tqdm((filtered_contexts), f\"Adding contexts of {content[\"table\"]} table\", leave=False)):\n",
    "            documents.append(context[\"context\"])\n",
    "            metadatas.append({\"table\": f\"{table}_SEP_{context_idx}\"})\n",
    "            ids.append(f\"{table}_SEP_{context_idx}\")\n",
    "\n",
    "    for i in range(0, len(documents), 30000):\n",
    "        embeddings = embedding_model.encode(\n",
    "            documents[i:i+30000],\n",
    "            batch_size=128,\n",
    "            show_progress_bar=True,\n",
    "            device=\"cuda\"\n",
    "        )\n",
    "\n",
    "        collection.add(\n",
    "            embeddings=[embed.tolist() for embed in embeddings],\n",
    "            metadatas=metadatas[i:i+30000],\n",
    "            ids=ids[i:i+30000]\n",
    "        )\n",
    "    return collection\n",
    "\n",
    "def indexing_keyword(\n",
    "    stemmer,\n",
    "    tables: list[str],\n",
    "    contexts: list[dict[str, str]],\n",
    "    narration_contents: list[dict[str, str]],\n",
    "):\n",
    "    corpus_json = []\n",
    "    for table in tqdm(tables, \"Adding contents\"):\n",
    "        cols_descriptions = [content[\"summary\"] for content in narration_contents if content[\"table\"] == table]\n",
    "        corpus_json.append({\"text\": \" | \".join(cols_descriptions), \"metadata\": {\"table\": f\"{table}_SEP_contents\"}})\n",
    "\n",
    "        filtered_contexts = [context for context in contexts if context[\"table\"] == table]\n",
    "        for context_idx, context in enumerate(tqdm((filtered_contexts), f\"Adding contexts of {table} table\", leave=False)):\n",
    "            corpus_json.append({\"text\": context[\"context\"], \"metadata\": {\"table\": f\"{table}_SEP_{context_idx}\"}})\n",
    "            ids_contexts[f\"{table}_SEP_{context_idx}\"] = context[\"context\"]\n",
    "\n",
    "    corpus_text = [doc[\"text\"] for doc in corpus_json]\n",
    "    corpus_tokens = bm25s.tokenize(corpus_text, stopwords=\"en\", stemmer=stemmer, show_progress=False)\n",
    "\n",
    "    retriever = bm25s.BM25(corpus=corpus_json)\n",
    "    retriever.index(corpus_tokens, show_progress=False)\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('dunzhang/stella_en_1.5B_v5', trust_remote_code=True)\n",
    "stemmer = Stemmer.Stemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.PersistentClient(f\"experiment-{dataset}\")\n",
    "collection = indexing_vector(client, model, contexts, std_contents)\n",
    "# collection = client.get_collection(\"benchmark\")\n",
    "retriever = indexing_keyword(stemmer, tables, contexts, narration_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_nodes_bm25(results, scores):\n",
    "    # Normalize relevance scores and return the nodes in dict format.\n",
    "    scores: list[float] = scores[0]\n",
    "    max_score = max(scores)\n",
    "    min_score = min(scores)\n",
    "\n",
    "    processed_nodes = {}\n",
    "    for i, node in enumerate(results[0]):\n",
    "        if min_score == max_score:\n",
    "            score = 1\n",
    "        else:\n",
    "            score = (scores[i] - min_score) / (max_score - min_score)\n",
    "        processed_nodes[node[\"metadata\"][\"table\"]] = score\n",
    "    return processed_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_nodes_vec(items):\n",
    "    # Normalize relevance scores and return the nodes in dict format.\n",
    "    scores: list[float] = [1 - dist for dist in items[\"distances\"][0]]\n",
    "    max_score = max(scores)\n",
    "    min_score = min(scores)\n",
    "\n",
    "    processed_nodes = {}\n",
    "    for idx, table in enumerate(items['metadatas'][0]):\n",
    "        if min_score == max_score:\n",
    "            score = 1\n",
    "        else:\n",
    "            score = (scores[idx] - min_score) / (max_score - min_score)\n",
    "        processed_nodes[table[\"table\"]] = score\n",
    "    return processed_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_retriever(bm25_res, bm25_sc, vec_res, k: int, question: str, ranking=False):\n",
    "    processed_nodes_bm25 = process_nodes_bm25(bm25_res, bm25_sc)\n",
    "    processed_nodes_vec: dict = process_nodes_vec(vec_res)\n",
    "\n",
    "    node_ids = set(list(processed_nodes_bm25.keys()) + list(processed_nodes_vec.keys()))\n",
    "    all_nodes = []\n",
    "    for node_id in node_ids:\n",
    "        bm25_score = processed_nodes_bm25.get(node_id, 0.0)\n",
    "        cosine_score = processed_nodes_vec.get(node_id, 0.0)\n",
    "        combined_score = 0.5 * bm25_score + 0.5 * cosine_score\n",
    "        all_nodes.append((node_id, combined_score))\n",
    "    \n",
    "    sorted_nodes = sorted(all_nodes, key=lambda node: (-node[1], node[0]))[:k]\n",
    "    if ranking:\n",
    "        reranked_nodes = rerank(sorted_nodes, question)\n",
    "        return reranked_nodes\n",
    "    return sorted_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_table_content_relevant(\n",
    "    path: str,\n",
    "    table: str,\n",
    "    max_tokens: int,\n",
    "    question: str\n",
    ") -> bool:\n",
    "    # # Context length of a model but reduced to account for other things (chat template, output token, etc.)\n",
    "    def find_largest_smaller_or_equal(tokens_list: list[int], max_tokens: int, pengurang: int):\n",
    "        for idx in range(len(tokens_list) - 1, -1, -1):\n",
    "            if (tokens_list[idx] - pengurang) <= max_tokens:\n",
    "                return idx\n",
    "        return -1\n",
    "\n",
    "    def get_processed_df(path: str, table: str) -> pd.DataFrame:\n",
    "        df = con.sql(f\"from '{path}/{table}.csv'\").to_df().drop_duplicates().reset_index(drop=True)\n",
    "        for col in df.columns:\n",
    "            if pd.api.types.is_datetime64_any_dtype(df[col]):\n",
    "                df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "                df[col] = df[col].apply(\n",
    "                    lambda x: x.strftime('%B ') + str(x.day).lstrip('0') + x.strftime(', %Y %H:%M:%S.%f')[:-3] if pd.notnull(x) else 'NaT'\n",
    "                )\n",
    "        return df\n",
    "\n",
    "    def get_content_relevancy_prompt(table: str, question: str):\n",
    "        return f\"\"\"Given this table:\n",
    "*/\n",
    "{table}\n",
    "*/\n",
    "and this question:\n",
    "/*\n",
    "{question}\n",
    "*/\n",
    "Is the table relevant to answer the question? Begin your answer with yes/no.\"\"\"\n",
    "\n",
    "    df = get_processed_df(path, table.split(\"_SEP_\")[0])\n",
    "    columns = \"col: \" + \" | \".join(df.columns)\n",
    "    rows = [\"\"] * len(df)\n",
    "\n",
    "    required_tokens = [len(pipe.tokenizer.tokenize(columns))] * len(df)\n",
    "\n",
    "    for row_idx, row in df.iterrows():\n",
    "        rows[row_idx] = f\"row {row_idx+1}: \" + \" | \".join(row.astype(str))\n",
    "        required_tokens[row_idx] += len(pipe.tokenizer.tokenize(rows[row_idx]))\n",
    "        if row_idx > 0:\n",
    "            required_tokens[row_idx] += required_tokens[row_idx-1]\n",
    "\n",
    "    last_processed_idx = 0\n",
    "    conversations: list[list[dict[str, str]]] = []\n",
    "    print(\"MASUK WHILE\")\n",
    "    pengurang = 0\n",
    "    while last_processed_idx < len(required_tokens):\n",
    "        to_process_idx = find_largest_smaller_or_equal(required_tokens[last_processed_idx:], max_tokens, pengurang)\n",
    "        if to_process_idx == -1:\n",
    "            return False\n",
    "\n",
    "        to_process_idx += last_processed_idx\n",
    "        prompt = get_content_relevancy_prompt(\n",
    "            columns + \"\\n\" + \"\\n\".join(rows[last_processed_idx:to_process_idx+1]),\n",
    "            question\n",
    "        )\n",
    "        conversations.append([{\"role\": \"user\", \"content\": prompt}])\n",
    "\n",
    "        last_processed_idx = to_process_idx + 1\n",
    "        pengurang += required_tokens[last_processed_idx-1] - len(pipe.tokenizer.tokenize(columns))\n",
    "\n",
    "    print(f\"DEBUG: {len(conversations)}\", flush=True)\n",
    "    for i in range(0, len(conversations), 4):\n",
    "        outputs = prompt_pipeline(\n",
    "            pipe, conversations[i:i+4], context_length=8192, max_new_tokens=3, top_p=None, temperature=None\n",
    "        )\n",
    "        for output in outputs:\n",
    "            answer: str = output[-1][\"content\"]\n",
    "            if answer.lower().startswith(\"yes\"):\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_table_context_relevant(context: str, question: str):\n",
    "    def get_context_relevancy_prompt(context: str, question: str):\n",
    "        return f\"\"\"Given this context describing a table:\n",
    "*/\n",
    "{context}\n",
    "*/\n",
    "and this question:\n",
    "/*\n",
    "{question}\n",
    "*/\n",
    "Is the table relevant to answer the question? Begin your answer with yes/no.\"\"\"\n",
    "    prompt = get_context_relevancy_prompt(\n",
    "        context,\n",
    "        question\n",
    "    )\n",
    "\n",
    "    answer: str = prompt_pipeline(\n",
    "        pipe, [[{\"role\": \"user\", \"content\": prompt}]], context_length=8192, max_new_tokens=3, top_p=None, temperature=None\n",
    "    )[0][-1][\"content\"]\n",
    "\n",
    "    if answer.lower().startswith(\"yes\"):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def rerank(nodes: list[tuple[str, str]], question: str):\n",
    "    max_tokens = 7000  # Context length of a model but reduced to account for other things (chat template, output token, etc.)\n",
    "    tables_relevancy = defaultdict(bool)\n",
    "\n",
    "    for node in tqdm(nodes, f\"Processing node...\"):\n",
    "        print(f\"Re-ranking node {node[0]}\", flush=True)\n",
    "        table_name = node[0]\n",
    "        if table_name.split(\"_SEP_\")[1] == \"contents\":\n",
    "            if is_table_content_relevant(path, table_name, max_tokens, question):\n",
    "                tables_relevancy[table_name] = True\n",
    "        else:\n",
    "            if is_table_context_relevant(ids_contexts[table_name], question):\n",
    "                tables_relevancy[table_name] = True\n",
    "    new_nodes = [(table_name, score) for table_name, score in nodes if tables_relevancy[table_name]] + [(table_name, score) for table_name, score in nodes if not tables_relevancy[table_name]]\n",
    "    return new_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_content_benchmark(\n",
    "    benchmark: list[dict[str,str]],\n",
    "    benchmark_type: str,\n",
    "    k: int,\n",
    "    model,\n",
    "    collection,\n",
    "    retriever,\n",
    "    stemmer,\n",
    "    use_reranker=False,\n",
    "    use_rephrased_questions=False\n",
    "):\n",
    "    hitrate_sum = 0\n",
    "    wrong_list = []\n",
    "\n",
    "    if use_reranker:\n",
    "        increased_k = k * 2\n",
    "    else:\n",
    "        increased_k = k\n",
    "    \n",
    "    def get_question_key(benchmark_type: str):\n",
    "        if benchmark_type == \"content\":\n",
    "            if not use_rephrased_questions:\n",
    "                question_key = \"question_from_sql_1\"\n",
    "            else:\n",
    "                question_key = \"question\"\n",
    "        else:\n",
    "            if not use_rephrased_questions:\n",
    "                question_key = \"question_bx1\"\n",
    "            else:\n",
    "                question_key = \"question_bx2\"\n",
    "        return question_key\n",
    "    question_key = get_question_key(benchmark_type)\n",
    "\n",
    "    questions = []\n",
    "    for data in benchmark:\n",
    "        questions.append(data[question_key])\n",
    "    embed_questions = model.encode(questions, batch_size=128, show_progress_bar=True)\n",
    "    embed_questions = [embed.tolist() for embed in embed_questions]\n",
    "\n",
    "    for idx, datum in enumerate(tqdm(benchmark)):\n",
    "        answer_tables = datum[\"answer_tables\"]\n",
    "        question_embedding = embed_questions[idx]\n",
    "\n",
    "        query_tokens = bm25s.tokenize(questions[idx], stemmer=stemmer, show_progress=False)\n",
    "        results, scores = retriever.retrieve(query_tokens, k=increased_k, show_progress=False)\n",
    "\n",
    "        vec_res = collection.query(\n",
    "            query_embeddings=[question_embedding],\n",
    "            n_results=increased_k\n",
    "        )\n",
    "\n",
    "        all_nodes = hybrid_retriever(results, scores, vec_res, increased_k, questions[idx], use_reranker)\n",
    "        before = hitrate_sum\n",
    "        for table, _ in all_nodes[:k]:\n",
    "            table = table.split(\"_SEP_\")[0]\n",
    "            if table in answer_tables:\n",
    "                hitrate_sum += 1\n",
    "                break\n",
    "        if before == hitrate_sum:\n",
    "            wrong_list.append(idx)\n",
    "        # Checkpoint\n",
    "        # if idx % 25 == 0:\n",
    "        #     print(f\"Current Hit Rate Sum: {hitrate_sum}\")\n",
    "    print(f\"Final Hit Rate Sum: {hitrate_sum}\")\n",
    "    print(f\"Hit Rate: {hitrate_sum/len(benchmark)}\")\n",
    "    print(f\"Wrong List: {wrong_list}\")\n",
    "    return hitrate_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BC1\n",
    "evaluate_content_benchmark(\n",
    "    content_benchmark, \"content\", 10, model, collection, retriever, stemmer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BC2\n",
    "evaluate_content_benchmark(\n",
    "    content_benchmark, \"content\", 10, model, collection, retriever, stemmer, use_rephrased_questions=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BX1\n",
    "evaluate_content_benchmark(\n",
    "    context_benchmark, \"context\", 10, model, collection, retriever, stemmer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BX2\n",
    "evaluate_content_benchmark(\n",
    "    context_benchmark, \"context\", 10, model, collection, retriever, stemmer, use_rephrased_questions=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summary-neo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
