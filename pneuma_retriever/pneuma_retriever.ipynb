{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import setproctitle\n",
    "import chromadb\n",
    "import bm25s\n",
    "import Stemmer\n",
    "import torch\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import set_seed\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.SentenceTransformer import SentenceTransformer\n",
    "from benchmark_generator.context.utils.jsonl import read_jsonl\n",
    "from benchmark_generator.context.utils.pipeline_initializer import initialize_pipeline\n",
    "from benchmark_generator.context.utils.prompting_interface import prompt_pipeline\n",
    "\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2\"\n",
    "set_seed(42, deterministic=True)\n",
    "setproctitle.setproctitle(\"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = initialize_pipeline(\"meta-llama/Meta-Llama-3-8B-Instruct\", torch.bfloat16)\n",
    "\n",
    "# Specific setting for Llama-3-8B-Instruct for batching\n",
    "pipe.tokenizer.pad_token_id = pipe.model.config.eos_token_id\n",
    "pipe.tokenizer.padding_side = 'left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer('dunzhang/stella_en_1.5B_v5', trust_remote_code=True)\n",
    "stemmer = Stemmer.Stemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"chicago\"\n",
    "if dataset == \"chicago\":\n",
    "    std_contents = read_jsonl(\"../pneuma_summarizer/summaries/standard/chicago_standard.jsonl\")\n",
    "    narration_contents = read_jsonl(\"../pneuma_summarizer/summaries/narrations/chicago_narrations.jsonl\")\n",
    "    contexts = read_jsonl(\"../data_src/benchmarks/context/chicago/contexts_chicago.jsonl\")\n",
    "\n",
    "    content_benchmark = read_jsonl(\"../data_src/benchmarks/content/pneuma_chicago_10K_questions_annotated.jsonl\")\n",
    "    context_benchmark = read_jsonl(\"../data_src/benchmarks/context/chicago/bx_chicago.jsonl\")\n",
    "    path = \"../data_src/tables/pneuma_chicago_10K\"\n",
    "elif dataset == \"public\":\n",
    "    std_contents = read_jsonl(\"../pneuma_summarizer/summaries/standard/public_standard.jsonl\")\n",
    "    narration_contents = read_jsonl(\"../pneuma_summarizer/summaries/narrations/public_narrations.jsonl\")\n",
    "    contexts = read_jsonl(\"../data_src/benchmarks/context/public/contexts_public.jsonl\")\n",
    "\n",
    "    content_benchmark = read_jsonl(\"../data_src/benchmarks/content/pneuma_public_bi_questions_annotated.jsonl\")\n",
    "    context_benchmark = read_jsonl(\"../data_src/benchmarks/context/public/bx_public.jsonl\")\n",
    "    path = \"../data_src/tables/pneuma_public_bi\"\n",
    "elif dataset == \"chembl\":\n",
    "    std_contents = read_jsonl(\"../pneuma_summarizer/summaries/standard/chembl_standard.jsonl\")\n",
    "    narration_contents = read_jsonl(\"../pneuma_summarizer/summaries/narrations/chembl_narrations.jsonl\")\n",
    "    contexts = read_jsonl(\"../data_src/benchmarks/context/chembl/contexts_chembl.jsonl\")\n",
    "\n",
    "    content_benchmark = read_jsonl(\"../data_src/benchmarks/content/pneuma_chembl_10K_questions_annotated.jsonl\")\n",
    "    context_benchmark = read_jsonl(\"../data_src/benchmarks/context/chembl/bx_chembl.jsonl\")\n",
    "    path = \"../data_src/tables/pneuma_chembl_10K\"\n",
    "elif dataset == \"adventure\":\n",
    "    std_contents = read_jsonl(\"../pneuma_summarizer/summaries/standard/adventure_standard.jsonl\")\n",
    "    narration_contents = read_jsonl(\"../pneuma_summarizer/summaries/narrations/adventure_narrations.jsonl\")\n",
    "    contexts = read_jsonl(\"../data_src/benchmarks/context/adventure/contexts_adventure.jsonl\")\n",
    "\n",
    "    content_benchmark = read_jsonl(\"../data_src/benchmarks/content/pneuma_adventure_works_questions_annotated.jsonl\")\n",
    "    context_benchmark = read_jsonl(\"../data_src/benchmarks/context/adventure/bx_adventure.jsonl\")\n",
    "    path = \"../data_src/tables/pneuma_adventure_works\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb.api.client import Client\n",
    "def indexing_vector(\n",
    "    client: Client,\n",
    "    embedding_model: SentenceTransformer,\n",
    "    std_contents: list[dict[str, str]],\n",
    "    contexts: list[dict[str, str]] = None,\n",
    "    collection_name = \"benchmark\",\n",
    "    reindex = False,\n",
    "):\n",
    "    documents = []\n",
    "    metadatas = []\n",
    "    ids = []\n",
    "\n",
    "    if not reindex:\n",
    "        try:\n",
    "            collection = client.get_collection(collection_name)\n",
    "            return collection\n",
    "        except:\n",
    "            pass\n",
    "    try:\n",
    "        client.delete_collection(collection_name)\n",
    "    except:\n",
    "        pass\n",
    "    collection = client.create_collection(\n",
    "        name=collection_name,\n",
    "        metadata={\"hnsw:space\": \"cosine\", \"hnsw:batch_size\": 1}\n",
    "    )\n",
    "\n",
    "    tables = sorted({content[\"table\"] for content in std_contents})\n",
    "    for table in tables:\n",
    "        cols = [content[\"summary\"] for content in std_contents if content[\"table\"] == table]\n",
    "        for content_idx, content in enumerate(cols):\n",
    "            documents.append(content)\n",
    "            metadatas.append({\"table\": f\"{table}_SEP_contents_{content_idx}\"})\n",
    "            ids.append(f\"{table}_SEP_contents_{content_idx}\")\n",
    "\n",
    "        if contexts is not None:\n",
    "            filtered_contexts = [context[\"context\"] for context in contexts if context[\"table\"] == table]\n",
    "            for context_idx, context in enumerate(filtered_contexts):\n",
    "                documents.append(context)\n",
    "                metadatas.append({\"table\": f\"{table}_SEP_{context_idx}\"})\n",
    "                ids.append(f\"{table}_SEP_{context_idx}\")\n",
    "\n",
    "    for i in range(0, len(documents), 30000):\n",
    "        embeddings = embedding_model.encode(\n",
    "            documents[i:i+30000],\n",
    "            batch_size=64,\n",
    "            show_progress_bar=True,\n",
    "            device=\"cuda\"\n",
    "        )\n",
    "\n",
    "        collection.add(\n",
    "            embeddings=[embed.tolist() for embed in embeddings],\n",
    "            metadatas=metadatas[i:i+30000],\n",
    "            documents=documents[i:i+30000],\n",
    "            ids=ids[i:i+30000],\n",
    "        )\n",
    "    return collection\n",
    "\n",
    "def indexing_keyword(\n",
    "    stemmer,\n",
    "    narration_contents: list[dict[str, str]],\n",
    "    contexts: list[dict[str, str]] = None,\n",
    "):\n",
    "    corpus_json = []\n",
    "    tables = sorted({content[\"table\"] for content in narration_contents})\n",
    "    for table in tables:\n",
    "        cols_descriptions = [content[\"summary\"] for content in narration_contents if content[\"table\"] == table]\n",
    "        for content_idx, content in enumerate(cols_descriptions):\n",
    "            corpus_json.append({\"text\": content, \"metadata\": {\"table\": f\"{table}_SEP_contents_{content_idx}\"}})\n",
    "\n",
    "        if contexts is not None:\n",
    "            filtered_contexts = [context[\"context\"] for context in contexts if context[\"table\"] == table]\n",
    "            for context_idx, context in enumerate(filtered_contexts):\n",
    "                corpus_json.append({\"text\": context, \"metadata\": {\"table\": f\"{table}_SEP_{context_idx}\"}})\n",
    "\n",
    "    corpus_text = [doc[\"text\"] for doc in corpus_json]\n",
    "    corpus_tokens = bm25s.tokenize(corpus_text, stopwords=\"en\", stemmer=stemmer, show_progress=False)\n",
    "\n",
    "    retriever = bm25s.BM25(corpus=corpus_json)\n",
    "    retriever.index(corpus_tokens, show_progress=False)\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.PersistentClient(f\"experiment-{dataset}\")\n",
    "collection = indexing_vector(client, embedding_model, std_contents, contexts)\n",
    "retriever = indexing_keyword(stemmer, narration_contents, contexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_nodes_bm25(items):\n",
    "    # Normalize relevance scores and return the nodes in dict format\n",
    "    results, scores = items\n",
    "    scores: list[float] = scores[0]\n",
    "    max_score = max(scores)\n",
    "    min_score = min(scores)\n",
    "\n",
    "    processed_nodes: dict[str,tuple[float,str]] = {}\n",
    "    for i, node in enumerate(results[0]):\n",
    "        if min_score == max_score:\n",
    "            score = 1\n",
    "        else:\n",
    "            score = (scores[i] - min_score) / (max_score - min_score)\n",
    "        processed_nodes[node[\"metadata\"][\"table\"]] = (score, node[\"text\"])\n",
    "    return processed_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_nodes_vec(items):\n",
    "    # Normalize relevance scores and return the nodes in dict format\n",
    "    scores: list[float] = [1 - dist for dist in items[\"distances\"][0]]\n",
    "    max_score = max(scores)\n",
    "    min_score = min(scores)\n",
    "\n",
    "    processed_nodes: dict[str,tuple[float, str]] = {}\n",
    "\n",
    "    for idx in range(len(items['ids'][0])):\n",
    "        if min_score == max_score:\n",
    "            score = 1\n",
    "        else:\n",
    "            score = (scores[idx] - min_score) / (max_score - min_score)\n",
    "        processed_nodes[items['ids'][0][idx]] = (score, items['documents'][0][idx])\n",
    "    return processed_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_retriever(\n",
    "    bm25_res,\n",
    "    vec_res,\n",
    "    k: int,\n",
    "    question: str,\n",
    "    embedding_model: SentenceTransformer,\n",
    "    use_reranker=False,\n",
    "):\n",
    "    processed_nodes_bm25 = process_nodes_bm25(bm25_res)\n",
    "    processed_nodes_vec = process_nodes_vec(vec_res)\n",
    "\n",
    "    node_ids = set(list(processed_nodes_bm25.keys()) + list(processed_nodes_vec.keys()))\n",
    "    all_nodes: list[tuple[str,float,str]] = []\n",
    "    for node_id in sorted(node_ids):\n",
    "        bm25_score_doc = processed_nodes_bm25.get(node_id, (0.0, None))\n",
    "        vec_score_doc = processed_nodes_vec.get(node_id, (0.0, None))\n",
    "\n",
    "        combined_score = 0.5 * bm25_score_doc[0] + 0.5 * vec_score_doc[0]\n",
    "        if bm25_score_doc[1] is None:\n",
    "            doc = vec_score_doc[1]\n",
    "        else:\n",
    "            doc = bm25_score_doc[1]\n",
    "\n",
    "        all_nodes.append((node_id, combined_score, doc))\n",
    "\n",
    "    sorted_nodes = sorted(all_nodes, key=lambda node: (-node[1], node[0]))[:k]\n",
    "    if use_reranker:\n",
    "        sorted_nodes = rerank(sorted_nodes, question, embedding_model)\n",
    "    return sorted_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_table_context_relevant(context: str, question: str):\n",
    "    prompt = f\"\"\"Given this context describing a table:\n",
    "*/\n",
    "{context}\n",
    "*/\n",
    "and this question:\n",
    "/*\n",
    "{question}\n",
    "*/\n",
    "Is the table relevant to answer the question? Begin your answer with yes/no.\"\"\"\n",
    "\n",
    "    answer: str = prompt_pipeline(\n",
    "        pipe, [[{\"role\": \"user\", \"content\": prompt}]], context_length=8192, max_new_tokens=3, top_p=None, temperature=None\n",
    "    )[0][-1][\"content\"]\n",
    "\n",
    "    if answer.lower().startswith(\"yes\"):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_table_content_relevant(content: str, question: str):\n",
    "    prompt = f\"\"\"Given a table with the following columns:\n",
    "*/\n",
    "{content}\n",
    "*/\n",
    "and this question:\n",
    "/*\n",
    "{question}\n",
    "*/\n",
    "Is the table relevant to answer the question? Begin your answer with yes/no.\"\"\"\n",
    "\n",
    "    answer: str = prompt_pipeline(\n",
    "        pipe, [[{\"role\": \"user\", \"content\": prompt}]], context_length=8192, max_new_tokens=3, top_p=None, temperature=None\n",
    "    )[0][-1][\"content\"]\n",
    "\n",
    "    if answer.lower().startswith(\"yes\"):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def rerank(nodes: list[tuple[str,float,str]], question: str, embedding_model: SentenceTransformer):\n",
    "    max_tokens = 7000  # Context length of a model but reduced to account for other things (chat template, output token, etc.)\n",
    "    tables_relevancy = defaultdict(bool)\n",
    "\n",
    "    for node in nodes:\n",
    "        table_name = node[0]\n",
    "        if table_name.split(\"_SEP_\")[1].startswith(\"contents\"):\n",
    "            # if is_table_content_relevant(table_name, question, max_tokens, embedding_model):\n",
    "            if is_table_content_relevant(node[2], question):\n",
    "                tables_relevancy[table_name] = True\n",
    "        else:\n",
    "            if is_table_context_relevant(node[2], question):\n",
    "                tables_relevancy[table_name] = True\n",
    "    new_nodes = [(table_name, score, doc) for table_name, score, doc in nodes if tables_relevancy[table_name]] + [(table_name, score, doc) for table_name, score, doc in nodes if not tables_relevancy[table_name]]\n",
    "    return new_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb.api.models.Collection import Collection\n",
    "def evaluate_benchmark(\n",
    "    benchmark: list[dict[str,str]],\n",
    "    benchmark_type: str,\n",
    "    k: int,\n",
    "    embedding_model: SentenceTransformer,\n",
    "    collection: Collection,\n",
    "    retriever,\n",
    "    stemmer,\n",
    "    use_reranker=False,\n",
    "    use_rephrased_questions=False\n",
    "):\n",
    "    hitrate_sum = 0\n",
    "    wrong_list = []\n",
    "\n",
    "    if use_reranker:\n",
    "        increased_k = k * 5\n",
    "    else:\n",
    "        increased_k = k\n",
    "    \n",
    "    def get_question_key(benchmark_type: str):\n",
    "        if benchmark_type == \"content\":\n",
    "            if not use_rephrased_questions:\n",
    "                question_key = \"question_from_sql_1\"\n",
    "            else:\n",
    "                question_key = \"question\"\n",
    "        else:\n",
    "            if not use_rephrased_questions:\n",
    "                question_key = \"question_bx1\"\n",
    "            else:\n",
    "                question_key = \"question_bx2\"\n",
    "        return question_key\n",
    "    question_key = get_question_key(benchmark_type)\n",
    "\n",
    "    questions = []\n",
    "    for data in benchmark:\n",
    "        questions.append(data[question_key])\n",
    "    embed_questions = embedding_model.encode(questions, batch_size=64, show_progress_bar=True)\n",
    "    embed_questions = [embed.tolist() for embed in embed_questions]\n",
    "\n",
    "    for idx, datum in enumerate(tqdm(benchmark)):\n",
    "        answer_tables = datum[\"answer_tables\"]\n",
    "        question_embedding = embed_questions[idx]\n",
    "\n",
    "        query_tokens = bm25s.tokenize(questions[idx], stemmer=stemmer, show_progress=False)\n",
    "        results, scores = retriever.retrieve(query_tokens, k=increased_k, show_progress=False)\n",
    "        bm25_res = (results, scores)\n",
    "\n",
    "        vec_res = collection.query(\n",
    "            query_embeddings=[question_embedding],\n",
    "            n_results=increased_k\n",
    "        )\n",
    "\n",
    "        all_nodes = hybrid_retriever(bm25_res, vec_res, increased_k, questions[idx], embedding_model, use_reranker)\n",
    "        before = hitrate_sum\n",
    "        for table,_,_ in all_nodes[:k]:\n",
    "            table = table.split(\"_SEP_\")[0]\n",
    "            if table in answer_tables:\n",
    "                hitrate_sum += 1\n",
    "                break\n",
    "        if before == hitrate_sum:\n",
    "            wrong_list.append(idx)\n",
    "        # Checkpoint\n",
    "        if idx % 5 == 0:\n",
    "            print(f\"Current Hit Rate Sum at index {idx}: {hitrate_sum}\")\n",
    "    print(f\"Final Hit Rate Sum: {hitrate_sum}\")\n",
    "    print(f\"Hit Rate: {hitrate_sum/len(benchmark)}\")\n",
    "    print(f\"Wrong List: {wrong_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = [1,5,10,30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in ks:\n",
    "    print(k)\n",
    "    evaluate_benchmark(\n",
    "        content_benchmark, \"content\", k, embedding_model, collection, retriever, stemmer, use_reranker=False\n",
    "    )\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in ks:\n",
    "    print(k)\n",
    "    evaluate_benchmark(\n",
    "        content_benchmark, \"content\", k, embedding_model, collection, retriever, stemmer, use_rephrased_questions=True, use_reranker=False\n",
    "    )\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in ks:\n",
    "    print(k)\n",
    "    evaluate_benchmark(\n",
    "        context_benchmark, \"context\", k, embedding_model, collection, retriever, stemmer, use_reranker=False\n",
    "    )\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in ks:\n",
    "    print(k)\n",
    "    evaluate_benchmark(\n",
    "        context_benchmark, \"context\", k, embedding_model, collection, retriever, stemmer,use_rephrased_questions=True, use_reranker=False\n",
    "    )\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BACKUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import duckdb\n",
    "# con = duckdb.connect()\n",
    "# def is_table_content_relevant(\n",
    "#     table: str,\n",
    "#     question: str,\n",
    "#     max_tokens: int,\n",
    "#     embedding_model: SentenceTransformer,\n",
    "#     num_of_rows=10,\n",
    "# ) -> bool:\n",
    "#     def find_largest_smaller_or_equal(tokens_list: list[int], max_tokens: int, aggregate_substractor: int):\n",
    "#         for idx in range(len(tokens_list) - 1, -1, -1):\n",
    "#             if (tokens_list[idx] - aggregate_substractor) <= max_tokens:\n",
    "#                 return idx\n",
    "#         return -1\n",
    "\n",
    "#     def get_processed_df(path: str, table: str) -> pd.DataFrame:\n",
    "#         df = con.sql(f\"from '{path}/{table}.csv'\").to_df().drop_duplicates().reset_index(drop=True)\n",
    "#         for col in df.columns:\n",
    "#             if pd.api.types.is_datetime64_any_dtype(df[col]):\n",
    "#                 df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "#                 df[col] = df[col].apply(\n",
    "#                     lambda x: x.strftime('%B ') + str(x.day).lstrip('0') + x.strftime(', %Y %H:%M:%S.%f')[:-3] if pd.notnull(x) else 'NaT'\n",
    "#                 )\n",
    "#         return df\n",
    "\n",
    "#     def get_content_relevancy_prompt(table: str, question: str):\n",
    "#         return f\"\"\"Given this table:\n",
    "# */\n",
    "# {table}\n",
    "# */\n",
    "# and this question:\n",
    "# /*\n",
    "# {question}\n",
    "# */\n",
    "# Is the table relevant to answer the question? Begin your answer with yes/no.\"\"\"\n",
    "\n",
    "#     df = get_processed_df(path, table.split(\"_SEP_\")[0])\n",
    "#     columns = \"col: \" + \" | \".join(df.columns)\n",
    "#     required_tokens = [len(pipe.tokenizer.tokenize(columns))] * min(num_of_rows, len(df))\n",
    "\n",
    "#     def get_relevant_rows(df: pd.DataFrame, question: str, num_of_rows=10):\n",
    "#         try:\n",
    "#             client = chromadb.PersistentClient(f\"experiment-{dataset}\")\n",
    "#             contents: list[dict[str,str]] = []\n",
    "#             for row_idx, row in df.iterrows():\n",
    "#                 contents.append({\n",
    "#                     \"table\": table,\n",
    "#                     \"summary\": f\"row {row_idx+1}: {\" | \".join(row.astype(str))}\"\n",
    "#                 })\n",
    "\n",
    "#             # Step 1: Indexing\n",
    "#             collection = indexing_vector(client, embedding_model, contents, collection_name=\"temporary\")\n",
    "#             retriever = indexing_keyword(stemmer, contents)\n",
    "\n",
    "#             # Step 2: Querying\n",
    "#             num_of_rows = min(num_of_rows, len(df))\n",
    "\n",
    "#             query_tokens = bm25s.tokenize(question, stemmer=stemmer, show_progress=False)\n",
    "#             results, scores = retriever.retrieve(query_tokens, k=num_of_rows, show_progress=False)\n",
    "#             bm25_res = (results, scores)\n",
    "\n",
    "#             vec_res = collection.query(\n",
    "#                 query_embeddings=[embedding_model.encode(question).tolist()],\n",
    "#                 n_results=num_of_rows\n",
    "#             )\n",
    "\n",
    "#             all_nodes = hybrid_retriever(\n",
    "#                 bm25_res,\n",
    "#                 vec_res,\n",
    "#                 num_of_rows,\n",
    "#                 question,\n",
    "#                 embedding_model\n",
    "#             )\n",
    "#             rows = []\n",
    "#             for node_idx, node in enumerate(all_nodes):\n",
    "#                 rows.append(node[2])\n",
    "#                 required_tokens[node_idx] += len(pipe.tokenizer.tokenize(node[2]))\n",
    "#                 if node_idx > 0:\n",
    "#                     required_tokens[node_idx] += required_tokens[node_idx-1]\n",
    "#             return rows\n",
    "#         except:\n",
    "#             return []\n",
    "\n",
    "#     rows = get_relevant_rows(df, question)\n",
    "\n",
    "#     last_unprocessed_idx = 0\n",
    "#     conversations: list[list[dict[str, str]]] = []\n",
    "#     aggregate_substractor = 0\n",
    "\n",
    "#     while last_unprocessed_idx < len(required_tokens):\n",
    "#         to_process_idx = find_largest_smaller_or_equal(\n",
    "#             required_tokens[last_unprocessed_idx:],\n",
    "#             max_tokens,\n",
    "#             aggregate_substractor\n",
    "#         )\n",
    "#         if to_process_idx == -1:\n",
    "#             return False\n",
    "\n",
    "#         to_process_idx += last_unprocessed_idx\n",
    "#         prompt = get_content_relevancy_prompt(\n",
    "#             columns + \"\\n\" + \"\\n\".join(rows[last_unprocessed_idx:to_process_idx+1]),\n",
    "#             question\n",
    "#         )\n",
    "#         conversations.append([{\"role\": \"user\", \"content\": prompt}])\n",
    "\n",
    "#         last_unprocessed_idx = to_process_idx + 1\n",
    "#         aggregate_substractor += (required_tokens[to_process_idx] - len(pipe.tokenizer.tokenize(columns)))\n",
    "\n",
    "#     for i in range(0, len(conversations), 2):\n",
    "#         outputs = prompt_pipeline(\n",
    "#             pipe, conversations[i:i+2], batch_size=2, context_length=8192, max_new_tokens=3, top_p=None, temperature=None\n",
    "#         )\n",
    "#         for output in outputs:\n",
    "#             answer: str = output[-1][\"content\"]\n",
    "#             if answer.lower().startswith(\"yes\"):\n",
    "#                 return True\n",
    "#     return False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summary-neo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
