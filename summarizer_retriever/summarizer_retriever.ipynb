{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Summarizing + Indexing to Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import setproctitle\n",
    "setproctitle.setproctitle(\"python\")\n",
    "from transformers import set_seed\n",
    "set_seed(42, deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "from llama_index.core import Document\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "from llama_index.core.node_parser import SentenceSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('dunzhang/stella_en_1.5B_v5', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.PersistentClient(\"testing\")\n",
    "collection = client.create_collection(name=\"benchmark\", metadata={\"hnsw:space\": \"cosine\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "con = duckdb.connect()\n",
    "path = \"../data_src/tables/public_bi_benchmark\"\n",
    "tables = [file[:-4] for file in sorted(os.listdir(path)) if file.endswith(\".csv\")]\n",
    "tables.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_method = \"column_names\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize for vector retriever\n",
    "def create_embed_meta_id(scenario: str):\n",
    "    embeddings = []\n",
    "    metadatas = []\n",
    "    ids = []\n",
    "    narrations = pd.read_csv(\"chicago_cols_descriptions.csv\")\n",
    "\n",
    "    for table in tqdm(tables):\n",
    "        df = con.sql(f\"select * from '{path}/{table}.csv'\").to_df()\n",
    "\n",
    "        if scenario == \"column_names\":\n",
    "            summary = \" | \".join(df.columns)\n",
    "            embedding = model.encode(summary)\n",
    "\n",
    "        elif scenario == \"row_values\":\n",
    "            try:\n",
    "                summary = ' | '.join(df.loc[0].astype(str))\n",
    "            except:\n",
    "                summary = \"\"\n",
    "            for i in range(1, len(df)):\n",
    "                summary += f\" || {' | '.join(df.loc[i].astype(str))}\"\n",
    "            chunks = chunk_text(summary, 131072)\n",
    "            chunk_embeds = [model.encode(chunk) for chunk in chunks]\n",
    "            embedding = np.mean(chunk_embeds, axis=0)\n",
    "\n",
    "        elif scenario == \"column_names_row_values\":\n",
    "            summary = \" | \".join(df.columns)\n",
    "            for i in range(len(df)):\n",
    "                summary += f\" || {' | '.join(df.loc[i].astype(str))}\"\n",
    "            chunks = chunk_text(summary, 131072)\n",
    "            chunk_embeds = [model.encode(chunk) for chunk in chunks]\n",
    "            embedding = np.mean(chunk_embeds, axis=0)\n",
    "\n",
    "        elif scenario == \"column_narration\":\n",
    "            narrations_filtered = narrations[narrations[\"table\"] == table].reset_index(drop=True)\n",
    "            summary = f\"{narrations_filtered[\"description\"][0]}\"\n",
    "            for i in range(1, len(narrations_filtered)):\n",
    "                summary += f\" | {narrations_filtered[\"description\"][i]}\"\n",
    "            embedding = model.encode(summary)\n",
    "\n",
    "        embeddings.append(embedding.tolist())\n",
    "        metadatas.append({\"table\": table})\n",
    "        ids.append(table)\n",
    "    return embeddings, metadatas, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing for vector retriever\n",
    "embeddings, metadatas, ids = create_embed_meta_id(summarize_method)\n",
    "collection.add(\n",
    "    embeddings=embeddings,\n",
    "    metadatas=metadatas,\n",
    "    ids=ids\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize for keyword retriever\n",
    "def create_documents(type_documents: str):\n",
    "    row_summaries = pd.read_csv(\"chicago_cols_descriptions.csv\")\n",
    "    documents = []\n",
    "\n",
    "    for table in tqdm(tables):\n",
    "        df = con.sql(f\"select * from '{path}/{table}.csv'\").to_df()\n",
    "\n",
    "        if type_documents == \"column_names\":\n",
    "            summary = \" | \".join(df.columns)\n",
    "\n",
    "        elif type_documents == \"column_narration\":\n",
    "            filtered_summaries = row_summaries[row_summaries[\"table\"] == table].reset_index(drop=True)\n",
    "            summary = f\"{filtered_summaries[\"description\"][0]}\"\n",
    "            for i in range(1, len(filtered_summaries)):\n",
    "                summary += f\" || {filtered_summaries[\"description\"][i]}\"\n",
    "\n",
    "        document = Document(\n",
    "            text=summary,\n",
    "            metadata={\"table\": table},\n",
    "            doc_id=table,\n",
    "        )\n",
    "        documents.append(document)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = create_documents(summarize_method)\n",
    "splitter = SentenceSplitter(paragraph_separator=\" || \")\n",
    "nodes = splitter.get_nodes_from_documents(documents)\n",
    "BM25_retriever = BM25Retriever.from_defaults(nodes=nodes, similarity_top_k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Benchmarking Retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def remove_stopwords(text: str):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def read_jsonl(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            data.append(json.loads(line.strip()))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_nodes_bm25(nodes):\n",
    "    # Normalize relevance scores and return the nodes in dict format.\n",
    "    scores: list[float] = [node.score for node in nodes]\n",
    "    max_score = max(scores)\n",
    "    min_score = min(scores)\n",
    "\n",
    "    processed_nodes = {}\n",
    "    for node in nodes:\n",
    "        if min_score == max_score:\n",
    "            node.score = 1\n",
    "        else:\n",
    "            node.score = (node.score - min_score) / (max_score - min_score)\n",
    "        processed_nodes[node.metadata[\"table\"]] = node.score\n",
    "    return processed_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_nodes_vec(items):\n",
    "    # Normalize relevance scores and return the nodes in dict format.\n",
    "    scores: list[float] = [1 - dist for dist in items[\"distances\"][0]]\n",
    "    max_score = max(scores)\n",
    "    min_score = min(scores)\n",
    "\n",
    "    processed_nodes = {}\n",
    "    for idx, table in enumerate(items['ids'][0]):\n",
    "        if min_score == max_score:\n",
    "            score = 1\n",
    "        else:\n",
    "            score = (scores[idx] - min_score) / (max_score - min_score)\n",
    "        processed_nodes[table] = score\n",
    "    return processed_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_hybrid_search(bm25_res, vec_res):\n",
    "    processed_nodes_bm25 = process_nodes_bm25(bm25_res)\n",
    "    processed_nodes_vec: dict = process_nodes_vec(vec_res)\n",
    "\n",
    "    node_ids = set(list(processed_nodes_bm25.keys()) + list(processed_nodes_vec.keys()))\n",
    "    all_nodes = []\n",
    "    for node_id in node_ids:\n",
    "        try:\n",
    "            bm25_score = processed_nodes_bm25.get(node_id, 0.0)\n",
    "        except:\n",
    "            bm25_score = 0.0\n",
    "        try:\n",
    "            cosine_score = processed_nodes_vec.get(node_id, 0.0)\n",
    "        except:\n",
    "            cosine_score = 0.0\n",
    "        combined_score = 0.5 * bm25_score + 0.5 * cosine_score\n",
    "        all_nodes.append((node_id, combined_score))\n",
    "    \n",
    "    sorted_nodes = sorted(all_nodes, key=lambda node: (-node[1], node[0]))[:10]\n",
    "    return sorted_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def evaluate_benchmark(jsonl_data, retrieval_type: str, k: int):\n",
    "    hitrate_sum = 0\n",
    "    wrong_list = []\n",
    "    i = 0\n",
    "    for datum in tqdm(jsonl_data):\n",
    "        answer_tables = datum[\"answer_tables\"]\n",
    "        if retrieval_type == \"keyword\":\n",
    "            BM25_retriever.similarity_top_k = k\n",
    "            question = remove_stopwords(datum[\"question\"])\n",
    "            results = BM25_retriever.retrieve(question)\n",
    "\n",
    "            before = hitrate_sum\n",
    "            for node in results:\n",
    "                table = node.metadata[\"table\"]\n",
    "                if table in answer_tables:\n",
    "                    hitrate_sum += 1\n",
    "                    break\n",
    "            if before == hitrate_sum:\n",
    "                wrong_list.append(i)\n",
    "        \n",
    "        elif retrieval_type == \"hybrid\":\n",
    "            question = datum[\"question\"]\n",
    "            question_embedding = model.encode(question).tolist()\n",
    "            stopwords_removed_question = remove_stopwords(question)\n",
    "            \n",
    "            bm25_res = BM25_retriever.retrieve(stopwords_removed_question)\n",
    "            vec_res = collection.query(\n",
    "                query_embeddings=[question_embedding],\n",
    "                n_results=k\n",
    "            )\n",
    "\n",
    "            all_nodes = process_hybrid_search(bm25_res, vec_res)\n",
    "            for table, _ in all_nodes:\n",
    "                if table in answer_tables:\n",
    "                    hitrate_sum += 1\n",
    "                    break\n",
    "\n",
    "        elif retrieval_type == \"vector\":\n",
    "            question = datum[\"question\"]\n",
    "            question_embedding = model.encode(question).tolist()\n",
    "            results = collection.query(\n",
    "                query_embeddings=[question_embedding],\n",
    "                n_results=k\n",
    "            )\n",
    "            before = hitrate_sum\n",
    "            for retrieved_table in results[\"metadatas\"][0]:\n",
    "                if retrieved_table['table'] in answer_tables:\n",
    "                    hitrate_sum += 1\n",
    "                    break\n",
    "            if before == hitrate_sum:\n",
    "                wrong_list.append(i)\n",
    "        else:\n",
    "            raise ValueError()\n",
    "        i += 1\n",
    "    print(f\"Hit Rate: {hitrate_sum/len(jsonl_data)}\")\n",
    "    with open(f\"{path}-{retrieval_type}-{method}-{k}-1.json\", 'w') as file:\n",
    "        json.dump(wrong_list, file)\n",
    "    return hitrate_sum/len(jsonl_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonl_path = \"public_bi_questions_annotated.jsonl\"\n",
    "jsonl_data = read_jsonl(jsonl_path)\n",
    "results = evaluate_benchmark(jsonl_data, \"keyword\", 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
