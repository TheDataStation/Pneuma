{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from pipeline.pipeline_initializer import initialize_pipeline\n",
    "from pipeline.prompting_interface import prompt_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = initialize_pipeline(\"meta-llama/Meta-Llama-3-8B-Instruct\", torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_prompt(question: str, answer: str):\n",
    "    \"\"\"Return a prompt string for zero shot scenario\n",
    "    \n",
    "    ## Parameters:\n",
    "        - affiliation (str): The affiliation of the dataset\n",
    "        - dataset (str): The dataset information\n",
    "        - question (str): A question to be asked about the dataset\n",
    "        - role (str): The role that the LLM play\n",
    "    \"\"\"\n",
    "    return f\"\"\"Question Q:\n",
    "/*\n",
    "{question}\n",
    "*/\n",
    "Answer A:\n",
    "/*\n",
    "{answer}\n",
    "*/\n",
    "Assume that the answerer has all the necessary information to respond to question Q. Evaluate answer A based on the following criteria:\n",
    "1. Completeness: The answer must definitively and comprehensively address all parts of question Q.\n",
    "2. Relevance: The answer must directly provide the information requested in question Q without any extraneous details.\n",
    "If the answer satisfies both criteria, label it as 'good'. If it fails to meet one or both criteria, label it as 'bad'. Provide your evaluation in the following format:\n",
    "- Label: [good/bad]\n",
    "- Reasoning: [Provide a brief explanation for your label]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"evaluate_ulang/openhermes-RP-nucleus_0.95.csv\"  # Adjust to the benchmark to be evaluated\n",
    "evals = pd.read_csv(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(evals.shape[0])):\n",
    "    table = evals[\"T\"][i]\n",
    "    question = evals[\"Q\"][i]\n",
    "    answer = evals[\"A\"][i]\n",
    "    curr_eval = evals[\"E\"][i]\n",
    "    if curr_eval == \"unknown\":\n",
    "        prompt = get_eval_prompt(question, answer)\n",
    "        conversation = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        model_output = prompt_pipeline(pipe, conversation)[-1][\"content\"]\n",
    "        if \"label: good\" in model_output.lower() or \"label: [good]\" in model_output.lower():\n",
    "            evaluation = \"good\"\n",
    "        elif \"label: bad\" in model_output.lower() or \"label: [bad]\" in model_output.lower():\n",
    "            evaluation = \"bad\"\n",
    "        else:\n",
    "            evaluation = model_output\n",
    "        evals.loc[i, \"E\"] = evaluation\n",
    "        evals.loc[i, \"R\"] = model_output\n",
    "        evals.to_csv(name, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
