{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from pipeline.pipeline_initializer import initialize_pipeline\n",
    "from pipeline.prompting_interface import prompt_pipeline\n",
    "from utils.csv_data_source import CsvDataSource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"resources/questions.json\") as file:\n",
    "    questions = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_standard(dataset: str, question: str):\n",
    "    return f\"\"\"Given this dataset:\n",
    "*/\n",
    "{dataset}\n",
    "*/\n",
    "and this question:\n",
    "/*\n",
    "{question}\n",
    "*/\n",
    "Assume you have all the necessary information to respond to the question. Generate an answer for the question given the dataset satisfying the following criteria:\n",
    "1. Completeness: The answer must definitively and comprehensively address all parts of the question.\n",
    "2. Relevance: The answer must directly provide the information requested in the question without any extraneous details.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_role_play(dataset: str, question: str, role: str):\n",
    "    return f\"\"\"Given this dataset:\n",
    "*/\n",
    "{dataset}\n",
    "*/\n",
    "and this question:\n",
    "/*\n",
    "{question}\n",
    "*/\n",
    "Assume you are {role} with all the necessary information to respond to the question. Generate an answer for the question given the dataset satisfying the following criteria:\n",
    "1. Completeness: The answer must definitively and comprehensively address all parts of the question.\n",
    "2. Relevance: The answer must directly provide the information requested in the question without any extraneous details.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"teknium/OpenHermes-2.5-Mistral-7B\"\n",
    "dtype = torch.bfloat16\n",
    "pipe = initialize_pipeline(model, dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_benchmark_standard(benchmark_name: str, generation_params={}):\n",
    "    csv_data_source = CsvDataSource(\"tables\")\n",
    "    for table in iter(csv_data_source):\n",
    "        try:\n",
    "            benchmark = pd.read_csv(benchmark_name)\n",
    "        except FileNotFoundError:\n",
    "            benchmark = pd.DataFrame(columns=['T','Q','A','E','R'])\n",
    "        csv_file_name = table[0]\n",
    "        print(f\"Processing table {csv_file_name}\")\n",
    "        dataset = \"\".join(table[1]).rstrip()\n",
    "        for i in questions.keys():\n",
    "            print(f\"Processing question {i}\")\n",
    "            question = questions[i][\"question\"]\n",
    "            prompt = get_prompt_standard(dataset, question)\n",
    "            conversation = [{\"role\": \"user\", \"content\": prompt}]\n",
    "            values = {'T': [csv_file_name[:-4]], 'Q': [question]}\n",
    "            if (benchmark[[\"T\",\"Q\"]].isin(values).all(axis=1).any()):\n",
    "                continue\n",
    "            answer = prompt_pipeline(pipe, conversation, **generation_params)[-1][\"content\"]\n",
    "            row = pd.DataFrame({\n",
    "                'T': [csv_file_name[:-4]],\n",
    "                'Q': [question],\n",
    "                'A': [answer],\n",
    "                'E': [\"unknown\"],\n",
    "                'R': [\"unknown\"],\n",
    "            })\n",
    "            benchmark = pd.concat([benchmark, row], ignore_index=True)\n",
    "            benchmark.to_csv(benchmark_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_benchmark_role_play(benchmark_name: str, generation_params={}):\n",
    "    csv_data_source = CsvDataSource(\"tables\")\n",
    "    for table in iter(csv_data_source):\n",
    "        try:\n",
    "            benchmark = pd.read_csv(benchmark_name)\n",
    "        except FileNotFoundError:\n",
    "            benchmark = pd.DataFrame(columns=['T','Q','A','E','R'])\n",
    "        csv_file_name = table[0]\n",
    "        print(f\"Processing table {csv_file_name}\")\n",
    "        dataset = \"\".join(table[1]).rstrip()\n",
    "        for i in questions.keys():\n",
    "            print(f\"Processing question {i}\")\n",
    "            question = questions[i][\"question\"]\n",
    "            role = questions[i][\"role\"]\n",
    "            prompt = get_prompt_role_play(dataset, question, role)\n",
    "            conversation = [{\"role\": \"user\", \"content\": prompt}]\n",
    "            values = {'T': [csv_file_name[:-4]], 'Q': [question]}\n",
    "            if (benchmark[[\"T\",\"Q\"]].isin(values).all(axis=1).any()):\n",
    "                continue\n",
    "            answer = prompt_pipeline(pipe, conversation, **generation_params)[-1][\"content\"]\n",
    "            row = pd.DataFrame({\n",
    "                'T': [csv_file_name[:-4]],\n",
    "                'Q': [question],\n",
    "                'A': [answer],\n",
    "                'E': [\"unknown\"],\n",
    "                'R': [\"unknown\"],\n",
    "            })\n",
    "            benchmark = pd.concat([benchmark, row], ignore_index=True)\n",
    "            benchmark.to_csv(benchmark_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greedy Search RP\n",
    "name = \"evaluate_ulang/solar-RP-greedy.csv\"\n",
    "generate_benchmark_role_play(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nucleus 0.90 RP\n",
    "# name = \"benchmarks/openhermes-RP-nucleus_0.90.csv\"\n",
    "# benchmark = generate_benchmark_role_play(name, {\"do_sample\": True, \"top_p\": 0.90, \"top_k\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nucleus 0.90 RP\n",
    "# name = \"benchmarks/openhermes-RP-contrastive_5_0.8.csv\"\n",
    "# benchmark = generate_benchmark_role_play(name, {\"do_sample\": True, \"top_k\": 5, \"penalty_alpha\": 0.8})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
